"""
Enhanced EEHFR WSNç³»ç»Ÿ - PSOç²’å­ç¾¤ä¼˜åŒ–æ¨¡å—
åŸºäºç”¨æˆ·è°ƒç ”æ–‡ä»¶ä¸­çš„æ··åˆå…ƒå¯å‘å¼ä¼˜åŒ–è®¾è®¡
å®ç°é«˜æ•ˆçš„ç°‡å¤´ä½ç½®å’Œèƒ½è€—ä¼˜åŒ–
"""

import numpy as np
import pandas as pd
from typing import List, Tuple, Dict, Optional
import matplotlib.pyplot as plt
from dataclasses import dataclass
import json

@dataclass
class Particle:
    """ç²’å­ç±»"""
    position: np.ndarray
    velocity: np.ndarray
    fitness: float
    best_position: np.ndarray
    best_fitness: float

class PSOOptimizer:
    """
    ç²’å­ç¾¤ä¼˜åŒ–ç®—æ³•
    ä¸“é—¨ç”¨äºWSNç°‡å¤´é€‰æ‹©å’Œèƒ½è€—ä¼˜åŒ–
    """
    
    def __init__(self, n_particles: int = 30, n_iterations: int = 100, 
                 w: float = 0.9, c1: float = 2.0, c2: float = 2.0,
                 w_decay: float = 0.95):
        """
        åˆå§‹åŒ–PSOä¼˜åŒ–å™¨
        
        Args:
            n_particles: ç²’å­æ•°é‡
            n_iterations: è¿­ä»£æ¬¡æ•°
            w: æƒ¯æ€§æƒé‡
            c1: ä¸ªä½“å­¦ä¹ å› å­
            c2: ç¤¾ä¼šå­¦ä¹ å› å­
            w_decay: æƒ¯æ€§æƒé‡è¡°å‡å› å­
        """
        self.n_particles = n_particles
        self.n_iterations = n_iterations
        self.w = w
        self.w_initial = w
        self.c1 = c1
        self.c2 = c2
        self.w_decay = w_decay
        
        # ä¼˜åŒ–å†å²
        self.optimization_history = []
        self.best_fitness_history = []
        self.convergence_data = []
        
        # æ€§èƒ½ç»Ÿè®¡
        self.total_evaluations = 0
        self.convergence_iteration = -1
        
    def _initialize_particles(self, n_nodes: int, n_clusters: int) -> List[Particle]:
        """åˆå§‹åŒ–ç²’å­ç¾¤"""
        particles = []
        
        for _ in range(self.n_particles):
            # éšæœºåˆå§‹åŒ–ç°‡å¤´ä½ç½®ï¼ˆèŠ‚ç‚¹ç´¢å¼•ï¼‰
            position = np.random.choice(n_nodes, n_clusters, replace=False)
            velocity = np.random.uniform(-1, 1, n_clusters)
            
            particle = Particle(
                position=position.astype(float),
                velocity=velocity,
                fitness=-np.inf,
                best_position=position.astype(float).copy(),
                best_fitness=-np.inf
            )
            particles.append(particle)
        
        return particles
    
    def _evaluate_fitness(self, particle_position: np.ndarray, 
                         nodes_data: np.ndarray) -> float:
        """
        è¯„ä¼°ç²’å­é€‚åº”åº¦
        
        Args:
            particle_position: ç²’å­ä½ç½®ï¼ˆç°‡å¤´èŠ‚ç‚¹ç´¢å¼•ï¼‰
            nodes_data: èŠ‚ç‚¹æ•°æ® [x, y, energy, ...]
            
        Returns:
            é€‚åº”åº¦å€¼ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰
        """
        self.total_evaluations += 1
        
        # ç¡®ä¿ç°‡å¤´ç´¢å¼•æœ‰æ•ˆ
        cluster_heads = np.clip(particle_position, 0, len(nodes_data) - 1).astype(int)
        cluster_heads = np.unique(cluster_heads)  # å»é™¤é‡å¤
        
        if len(cluster_heads) < 2:
            return -1000  # æƒ©ç½šæ— æ•ˆè§£
        
        # 1. èƒ½é‡å‡è¡¡æ€§è¯„ä¼°
        ch_energies = nodes_data[cluster_heads, 2]  # å‡è®¾ç¬¬3åˆ—æ˜¯èƒ½é‡
        energy_balance = 1.0 / (1.0 + np.std(ch_energies))
        
        # 2. ç°‡å†…è·ç¦»è¯„ä¼°ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
        total_intra_distance = 0
        for ch_idx in cluster_heads:
            ch_pos = nodes_data[ch_idx, :2]  # x, yåæ ‡
            distances = np.sqrt(np.sum((nodes_data[:, :2] - ch_pos)**2, axis=1))
            total_intra_distance += np.mean(distances)
        
        avg_intra_distance = total_intra_distance / len(cluster_heads)
        distance_score = 1.0 / (1.0 + avg_intra_distance)
        
        # 3. ç°‡å¤´é—´è·ç¦»è¯„ä¼°ï¼ˆé¿å…è¿‡äºé›†ä¸­ï¼‰
        ch_positions = nodes_data[cluster_heads, :2]
        inter_distances = []
        for i in range(len(cluster_heads)):
            for j in range(i + 1, len(cluster_heads)):
                dist = np.sqrt(np.sum((ch_positions[i] - ch_positions[j])**2))
                inter_distances.append(dist)
        
        if inter_distances:
            avg_inter_distance = np.mean(inter_distances)
            separation_score = min(1.0, avg_inter_distance / 50.0)  # å½’ä¸€åŒ–
        else:
            separation_score = 0.0
        
        # 4. èƒ½é‡æ•ˆç‡è¯„ä¼°
        total_energy = np.sum(ch_energies)
        energy_efficiency = total_energy / len(cluster_heads)
        
        # ç»¼åˆé€‚åº”åº¦è®¡ç®—
        fitness = (0.3 * energy_balance + 
                  0.3 * distance_score + 
                  0.2 * separation_score + 
                  0.2 * energy_efficiency)
        
        return fitness
    
    def _update_particle(self, particle: Particle, global_best_position: np.ndarray):
        """æ›´æ–°ç²’å­ä½ç½®å’Œé€Ÿåº¦"""
        r1, r2 = np.random.rand(2)
        
        # æ›´æ–°é€Ÿåº¦
        particle.velocity = (self.w * particle.velocity + 
                           self.c1 * r1 * (particle.best_position - particle.position) + 
                           self.c2 * r2 * (global_best_position - particle.position))
        
        # é™åˆ¶é€Ÿåº¦
        max_velocity = 5.0
        particle.velocity = np.clip(particle.velocity, -max_velocity, max_velocity)
        
        # æ›´æ–°ä½ç½®
        particle.position = particle.position + particle.velocity
        
        # ç¡®ä¿ä½ç½®åœ¨æœ‰æ•ˆèŒƒå›´å†…
        particle.position = np.clip(particle.position, 0, len(particle.position) - 1)
    
    def optimize_cluster_heads(self, nodes_data: np.ndarray, 
                             n_clusters: int = 6) -> Tuple[np.ndarray, float]:
        """
        ä¼˜åŒ–ç°‡å¤´é€‰æ‹©
        
        Args:
            nodes_data: èŠ‚ç‚¹æ•°æ®æ•°ç»„ [x, y, energy, ...]
            n_clusters: ç°‡å¤´æ•°é‡
            
        Returns:
            (æœ€ä¼˜ç°‡å¤´ç´¢å¼•, æœ€ä¼˜é€‚åº”åº¦)
        """
        
        print(f"ğŸ”„ å¯åŠ¨PSOä¼˜åŒ– - ç²’å­æ•°: {self.n_particles}, è¿­ä»£æ•°: {self.n_iterations}")
        
        # åˆå§‹åŒ–ç²’å­ç¾¤
        particles = self._initialize_particles(len(nodes_data), n_clusters)
        
        # å…¨å±€æœ€ä¼˜
        global_best_position = None
        global_best_fitness = -np.inf
        
        # æ”¶æ•›æ£€æµ‹
        stagnation_count = 0
        convergence_threshold = 1e-6
        
        for iteration in range(self.n_iterations):
            iteration_best_fitness = -np.inf
            
            # è¯„ä¼°æ‰€æœ‰ç²’å­
            for particle in particles:
                fitness = self._evaluate_fitness(particle.position, nodes_data)
                particle.fitness = fitness
                
                # æ›´æ–°ä¸ªä½“æœ€ä¼˜
                if fitness > particle.best_fitness:
                    particle.best_fitness = fitness
                    particle.best_position = particle.position.copy()
                
                # æ›´æ–°å…¨å±€æœ€ä¼˜
                if fitness > global_best_fitness:
                    global_best_fitness = fitness
                    global_best_position = particle.position.copy()
                
                iteration_best_fitness = max(iteration_best_fitness, fitness)
            
            # æ›´æ–°æ‰€æœ‰ç²’å­
            for particle in particles:
                self._update_particle(particle, global_best_position)
            
            # æ›´æ–°æƒ¯æ€§æƒé‡
            self.w = max(0.1, self.w * self.w_decay)
            
            # è®°å½•æ”¶æ•›æ•°æ®
            self.best_fitness_history.append(global_best_fitness)
            self.convergence_data.append({
                'iteration': iteration,
                'best_fitness': global_best_fitness,
                'avg_fitness': np.mean([p.fitness for p in particles]),
                'diversity': self._calculate_diversity(particles)
            })
            
            # æ”¶æ•›æ£€æµ‹
            if iteration > 10:
                improvement = (self.best_fitness_history[-1] - 
                             self.best_fitness_history[-10])
                if improvement < convergence_threshold:
                    stagnation_count += 1
                else:
                    stagnation_count = 0
                
                if stagnation_count >= 20:
                    self.convergence_iteration = iteration
                    print(f"âœ… PSOåœ¨ç¬¬{iteration}è½®æ”¶æ•›")
                    break
            
            if iteration % 20 == 0:
                print(f"   è¿­ä»£ {iteration}: æœ€ä¼˜é€‚åº”åº¦ = {global_best_fitness:.6f}")
        
        # è½¬æ¢ä¸ºæ•´æ•°ç´¢å¼•
        optimal_cluster_heads = np.clip(global_best_position, 0, 
                                      len(nodes_data) - 1).astype(int)
        optimal_cluster_heads = np.unique(optimal_cluster_heads)
        
        print(f"âœ… PSOä¼˜åŒ–å®Œæˆ - æœ€ä¼˜é€‚åº”åº¦: {global_best_fitness:.6f}")
        print(f"   é€‰æ‹©çš„ç°‡å¤´: {optimal_cluster_heads}")
        
        return optimal_cluster_heads, global_best_fitness
    
    def _calculate_diversity(self, particles: List[Particle]) -> float:
        """è®¡ç®—ç²’å­ç¾¤å¤šæ ·æ€§"""
        positions = np.array([p.position for p in particles])
        center = np.mean(positions, axis=0)
        diversity = np.mean([np.linalg.norm(pos - center) for pos in positions])
        return diversity
    
    def visualize_convergence(self, save_path: str = None) -> plt.Figure:
        """å¯è§†åŒ–æ”¶æ•›è¿‡ç¨‹"""
        if not self.convergence_data:
            print("âŒ æ²¡æœ‰æ”¶æ•›æ•°æ®å¯è§†åŒ–")
            return None
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        iterations = [d['iteration'] for d in self.convergence_data]
        best_fitness = [d['best_fitness'] for d in self.convergence_data]
        avg_fitness = [d['avg_fitness'] for d in self.convergence_data]
        diversity = [d['diversity'] for d in self.convergence_data]
        
        # é€‚åº”åº¦æ”¶æ•›æ›²çº¿
        axes[0, 0].plot(iterations, best_fitness, 'b-', label='æœ€ä¼˜é€‚åº”åº¦', linewidth=2)
        axes[0, 0].plot(iterations, avg_fitness, 'r--', label='å¹³å‡é€‚åº”åº¦', linewidth=2)
        axes[0, 0].set_title('PSOé€‚åº”åº¦æ”¶æ•›æ›²çº¿')
        axes[0, 0].set_xlabel('è¿­ä»£æ¬¡æ•°')
        axes[0, 0].set_ylabel('é€‚åº”åº¦')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # å¤šæ ·æ€§å˜åŒ–
        axes[0, 1].plot(iterations, diversity, 'g-', linewidth=2)
        axes[0, 1].set_title('ç²’å­ç¾¤å¤šæ ·æ€§å˜åŒ–')
        axes[0, 1].set_xlabel('è¿­ä»£æ¬¡æ•°')
        axes[0, 1].set_ylabel('å¤šæ ·æ€§')
        axes[0, 1].grid(True, alpha=0.3)
        
        # é€‚åº”åº¦åˆ†å¸ƒç›´æ–¹å›¾
        final_fitness = [d['best_fitness'] for d in self.convergence_data[-10:]]
        axes[1, 0].hist(final_fitness, bins=20, alpha=0.7, color='purple')
        axes[1, 0].set_title('æœ€ç»ˆé€‚åº”åº¦åˆ†å¸ƒ')
        axes[1, 0].set_xlabel('é€‚åº”åº¦')
        axes[1, 0].set_ylabel('é¢‘æ¬¡')
        
        # æ”¶æ•›ç»Ÿè®¡
        convergence_info = f"""
        æ€»è¿­ä»£æ¬¡æ•°: {len(iterations)}
        æ”¶æ•›è½®æ¬¡: {self.convergence_iteration if self.convergence_iteration > 0 else 'æœªæ”¶æ•›'}
        æœ€ä¼˜é€‚åº”åº¦: {max(best_fitness):.6f}
        æ€»è¯„ä¼°æ¬¡æ•°: {self.total_evaluations}
        """
        
        axes[1, 1].text(0.1, 0.5, convergence_info, fontsize=12, 
                        verticalalignment='center', transform=axes[1, 1].transAxes)
        axes[1, 1].set_title('ä¼˜åŒ–ç»Ÿè®¡ä¿¡æ¯')
        axes[1, 1].axis('off')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… PSOæ”¶æ•›å›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")
        
        return fig
    
    def get_optimization_summary(self) -> Dict:
        """è·å–ä¼˜åŒ–æ€»ç»“"""
        if not self.convergence_data:
            return {"message": "æš‚æ— ä¼˜åŒ–æ•°æ®"}
        
        summary = {
            "algorithm": "Particle Swarm Optimization",
            "parameters": {
                "n_particles": self.n_particles,
                "n_iterations": self.n_iterations,
                "w_initial": self.w_initial,
                "c1": self.c1,
                "c2": self.c2,
                "w_decay": self.w_decay
            },
            "performance": {
                "total_evaluations": self.total_evaluations,
                "convergence_iteration": self.convergence_iteration,
                "best_fitness": max(self.best_fitness_history) if self.best_fitness_history else 0,
                "final_diversity": self.convergence_data[-1]['diversity'] if self.convergence_data else 0
            },
            "convergence_data": self.convergence_data[-10:]  # æœ€å10è½®æ•°æ®
        }
        
        return summary
    
    def save_results(self, save_path: str):
        """ä¿å­˜ä¼˜åŒ–ç»“æœ"""
        summary = self.get_optimization_summary()
        
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… PSOä¼˜åŒ–ç»“æœå·²ä¿å­˜åˆ°: {save_path}")

if __name__ == "__main__":
    # æµ‹è¯•PSOä¼˜åŒ–å™¨
    pso = PSOOptimizer(n_particles=20, n_iterations=50)
    
    # åˆ›å»ºæµ‹è¯•èŠ‚ç‚¹æ•°æ®
    np.random.seed(42)
    n_nodes = 30
    nodes_data = np.column_stack([
        np.random.uniform(0, 100, n_nodes),  # xåæ ‡
        np.random.uniform(0, 100, n_nodes),  # yåæ ‡
        np.random.uniform(0.3, 1.0, n_nodes)  # èƒ½é‡
    ])
    
    # è¿è¡Œä¼˜åŒ–
    optimal_heads, best_fitness = pso.optimize_cluster_heads(nodes_data, n_clusters=6)
    
    print(f"æœ€ä¼˜ç°‡å¤´: {optimal_heads}")
    print(f"æœ€ä¼˜é€‚åº”åº¦: {best_fitness}")
    
    # å¯è§†åŒ–ç»“æœ
    pso.visualize_convergence("pso_convergence.png")
    
    # ä¿å­˜ç»“æœ
    pso.save_results("pso_results.json")