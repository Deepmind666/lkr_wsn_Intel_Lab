"""
Enhanced EEHFR WSNç³»ç»Ÿ - LSTMæ—¶åºé¢„æµ‹æ¨¡å—
åŸºäºç”¨æˆ·è°ƒç ”æ–‡ä»¶ä¸­çš„è½»é‡çº§æ—¶åºé¢„æµ‹æ¨¡å‹è®¾è®¡
å®ç°ä¼ æ„Ÿå™¨æ•°æ®é¢„æµ‹å’Œå¼‚å¸¸æ£€æµ‹
"""

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from typing import List, Tuple, Dict, Optional
import matplotlib.pyplot as plt
import seaborn as sns
import json
import warnings
warnings.filterwarnings('ignore')

class LSTMPredictor(nn.Module):
    """
    è½»é‡çº§LSTMé¢„æµ‹æ¨¡å‹
    ä¸“é—¨ç”¨äºWSNä¼ æ„Ÿå™¨æ•°æ®æ—¶åºé¢„æµ‹
    """
    
    def __init__(self, input_size: int = 4, hidden_size: int = 64, 
                 num_layers: int = 2, output_size: int = 4, 
                 dropout: float = 0.2, bidirectional: bool = False):
        """
        åˆå§‹åŒ–LSTMé¢„æµ‹å™¨
        
        Args:
            input_size: è¾“å…¥ç‰¹å¾ç»´åº¦
            hidden_size: éšè—å±‚å¤§å°
            num_layers: LSTMå±‚æ•°
            output_size: è¾“å‡ºç»´åº¦
            dropout: Dropoutæ¯”ä¾‹
            bidirectional: æ˜¯å¦ä½¿ç”¨åŒå‘LSTM
        """
        super(LSTMPredictor, self).__init__()
        
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.output_size = output_size
        self.bidirectional = bidirectional
        
        # LSTMå±‚
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0,
            bidirectional=bidirectional
        )
        
        # æ³¨æ„åŠ›æœºåˆ¶
        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_size * (2 if bidirectional else 1),
            num_heads=4,
            dropout=dropout,
            batch_first=True
        )
        
        # å…¨è¿æ¥å±‚
        lstm_output_size = hidden_size * (2 if bidirectional else 1)
        self.fc_layers = nn.Sequential(
            nn.Linear(lstm_output_size, lstm_output_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(lstm_output_size // 2, lstm_output_size // 4),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(lstm_output_size // 4, output_size)
        )
        
        # æ®‹å·®è¿æ¥ï¼ˆå¦‚æœè¾“å…¥è¾“å‡ºç»´åº¦ç›¸åŒï¼‰
        self.use_residual = (input_size == output_size)
        if self.use_residual:
            self.residual_projection = nn.Linear(input_size, output_size)
    
    def forward(self, x):
        batch_size, seq_len, _ = x.size()
        
        # LSTMå‰å‘ä¼ æ’­
        lstm_out, (hidden, cell) = self.lstm(x)
        
        # æ³¨æ„åŠ›æœºåˆ¶
        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)
        
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        final_output = attn_out[:, -1, :]
        
        # å…¨è¿æ¥å±‚
        prediction = self.fc_layers(final_output)
        
        # æ®‹å·®è¿æ¥
        if self.use_residual:
            residual = self.residual_projection(x[:, -1, :])
            prediction = prediction + residual
        
        return prediction

class WSNDataPreprocessor:
    """WSNæ•°æ®é¢„å¤„ç†å™¨"""
    
    def __init__(self, sequence_length: int = 10, prediction_horizon: int = 1):
        self.sequence_length = sequence_length
        self.prediction_horizon = prediction_horizon
        self.scalers = {}
        self.feature_names = []
        
    def fit_transform(self, data: pd.DataFrame, 
                     feature_columns: List[str]) -> Tuple[np.ndarray, np.ndarray]:
        """æ‹Ÿåˆå¹¶è½¬æ¢æ•°æ®"""
        self.feature_names = feature_columns
        
        # æ•°æ®æ¸…æ´—
        cleaned_data = self._clean_data(data, feature_columns)
        
        # ç‰¹å¾ç¼©æ”¾
        scaled_data = self._scale_features(cleaned_data, feature_columns)
        
        # åˆ›å»ºåºåˆ—
        sequences, targets = self._create_sequences(scaled_data, feature_columns)
        
        return sequences, targets
    
    def transform(self, data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """è½¬æ¢æ–°æ•°æ®"""
        # æ•°æ®æ¸…æ´—
        cleaned_data = self._clean_data(data, self.feature_names)
        
        # ç‰¹å¾ç¼©æ”¾
        scaled_data = self._transform_features(cleaned_data, self.feature_names)
        
        # åˆ›å»ºåºåˆ—
        sequences, targets = self._create_sequences(scaled_data, self.feature_names)
        
        return sequences, targets
    
    def _clean_data(self, data: pd.DataFrame, feature_columns: List[str]) -> pd.DataFrame:
        """æ•°æ®æ¸…æ´—"""
        cleaned = data.copy()
        
        # ç§»é™¤å¼‚å¸¸å€¼
        for col in feature_columns:
            Q1 = cleaned[col].quantile(0.25)
            Q3 = cleaned[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            cleaned = cleaned[(cleaned[col] >= lower_bound) & 
                            (cleaned[col] <= upper_bound)]
        
        # å¡«å……ç¼ºå¤±å€¼
        cleaned[feature_columns] = cleaned[feature_columns].fillna(
            cleaned[feature_columns].mean())
        
        return cleaned
    
    def _scale_features(self, data: pd.DataFrame, 
                       feature_columns: List[str]) -> pd.DataFrame:
        """ç‰¹å¾ç¼©æ”¾"""
        scaled_data = data.copy()
        
        for col in feature_columns:
            scaler = MinMaxScaler()
            scaled_data[col] = scaler.fit_transform(data[[col]])
            self.scalers[col] = scaler
        
        return scaled_data
    
    def _transform_features(self, data: pd.DataFrame, 
                          feature_columns: List[str]) -> pd.DataFrame:
        """è½¬æ¢ç‰¹å¾"""
        scaled_data = data.copy()
        
        for col in feature_columns:
            if col in self.scalers:
                scaled_data[col] = self.scalers[col].transform(data[[col]])
        
        return scaled_data
    
    def _create_sequences(self, data: pd.DataFrame, 
                         feature_columns: List[str]) -> Tuple[np.ndarray, np.ndarray]:
        """åˆ›å»ºæ—¶åºåºåˆ—"""
        sequences = []
        targets = []
        
        # æŒ‰èŠ‚ç‚¹åˆ†ç»„
        if 'moteid' in data.columns:
            for node_id in data['moteid'].unique():
                node_data = data[data['moteid'] == node_id][feature_columns].values
                
                if len(node_data) >= self.sequence_length + self.prediction_horizon:
                    for i in range(len(node_data) - self.sequence_length - self.prediction_horizon + 1):
                        seq = node_data[i:i + self.sequence_length]
                        target = node_data[i + self.sequence_length:i + self.sequence_length + self.prediction_horizon]
                        
                        sequences.append(seq)
                        targets.append(target.flatten() if self.prediction_horizon > 1 else target[0])
        else:
            # å•èŠ‚ç‚¹æ•°æ®
            node_data = data[feature_columns].values
            for i in range(len(node_data) - self.sequence_length - self.prediction_horizon + 1):
                seq = node_data[i:i + self.sequence_length]
                target = node_data[i + self.sequence_length:i + self.sequence_length + self.prediction_horizon]
                
                sequences.append(seq)
                targets.append(target.flatten() if self.prediction_horizon > 1 else target[0])
        
        return np.array(sequences), np.array(targets)
    
    def inverse_transform(self, scaled_data: np.ndarray, 
                         feature_name: str) -> np.ndarray:
        """åå‘è½¬æ¢æ•°æ®"""
        if feature_name in self.scalers:
            return self.scalers[feature_name].inverse_transform(
                scaled_data.reshape(-1, 1)).flatten()
        return scaled_data

class LSTMTrainer:
    """LSTMè®­ç»ƒå™¨"""
    
    def __init__(self, model: LSTMPredictor, device: str = 'cpu'):
        self.model = model.to(device)
        self.device = device
        self.training_history = []
        self.best_model_state = None
        self.best_loss = float('inf')
        
    def train(self, train_loader: DataLoader, val_loader: DataLoader,
              epochs: int = 100, learning_rate: float = 0.001,
              patience: int = 10) -> Dict:
        """è®­ç»ƒæ¨¡å‹"""
        
        criterion = nn.MSELoss()
        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=5, verbose=True)
        
        train_losses = []
        val_losses = []
        early_stop_counter = 0
        
        print(f"ğŸ”„ å¼€å§‹LSTMè®­ç»ƒ - è®¾å¤‡: {self.device}")
        
        for epoch in range(epochs):
            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            train_loss = 0.0
            
            for batch_x, batch_y in train_loader:
                batch_x = batch_x.to(self.device)
                batch_y = batch_y.to(self.device)
                
                optimizer.zero_grad()
                outputs = self.model(batch_x)
                loss = criterion(outputs, batch_y)
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                optimizer.step()
                train_loss += loss.item()
            
            avg_train_loss = train_loss / len(train_loader)
            train_losses.append(avg_train_loss)
            
            # éªŒè¯é˜¶æ®µ
            self.model.eval()
            val_loss = 0.0
            
            with torch.no_grad():
                for batch_x, batch_y in val_loader:
                    batch_x = batch_x.to(self.device)
                    batch_y = batch_y.to(self.device)
                    
                    outputs = self.model(batch_x)
                    loss = criterion(outputs, batch_y)
                    val_loss += loss.item()
            
            avg_val_loss = val_loss / len(val_loader)
            val_losses.append(avg_val_loss)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step(avg_val_loss)
            
            # æ—©åœæ£€æŸ¥
            if avg_val_loss < self.best_loss:
                self.best_loss = avg_val_loss
                self.best_model_state = self.model.state_dict().copy()
                early_stop_counter = 0
            else:
                early_stop_counter += 1
            
            # è®°å½•è®­ç»ƒå†å²
            self.training_history.append({
                'epoch': epoch,
                'train_loss': avg_train_loss,
                'val_loss': avg_val_loss,
                'lr': optimizer.param_groups[0]['lr']
            })
            
            if epoch % 10 == 0:
                print(f"   Epoch {epoch}: Train Loss = {avg_train_loss:.6f}, "
                      f"Val Loss = {avg_val_loss:.6f}")
            
            # æ—©åœ
            if early_stop_counter >= patience:
                print(f"âœ… æ—©åœäºç¬¬ {epoch} è½®")
                break
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        if self.best_model_state:
            self.model.load_state_dict(self.best_model_state)
        
        training_stats = {
            'total_epochs': len(train_losses),
            'best_val_loss': self.best_loss,
            'final_train_loss': train_losses[-1],
            'final_val_loss': val_losses[-1],
            'training_history': self.training_history
        }
        
        print(f"âœ… LSTMè®­ç»ƒå®Œæˆ - æœ€ä½³éªŒè¯æŸå¤±: {self.best_loss:.6f}")
        
        return training_stats
    
    def evaluate(self, test_loader: DataLoader) -> Dict:
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        
        all_predictions = []
        all_targets = []
        
        with torch.no_grad():
            for batch_x, batch_y in test_loader:
                batch_x = batch_x.to(self.device)
                batch_y = batch_y.to(self.device)
                
                outputs = self.model(batch_x)
                
                all_predictions.append(outputs.cpu().numpy())
                all_targets.append(batch_y.cpu().numpy())
        
        predictions = np.vstack(all_predictions)
        targets = np.vstack(all_targets)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        mae = mean_absolute_error(targets, predictions)
        mse = mean_squared_error(targets, predictions)
        rmse = np.sqrt(mse)
        r2 = r2_score(targets, predictions)
        
        # è®¡ç®—MAPE
        mape = np.mean(np.abs((targets - predictions) / (targets + 1e-10))) * 100
        
        metrics = {
            'mae': mae,
            'mse': mse,
            'rmse': rmse,
            'r2': r2,
            'mape': mape,
            'predictions': predictions,
            'targets': targets
        }
        
        print(f"âœ… æ¨¡å‹è¯„ä¼°å®Œæˆ:")
        print(f"   MAE: {mae:.6f}")
        print(f"   RMSE: {rmse:.6f}")
        print(f"   RÂ²: {r2:.6f}")
        print(f"   MAPE: {mape:.2f}%")
        
        return metrics

class WSNLSTMSystem:
    """WSN LSTMé¢„æµ‹ç³»ç»Ÿ"""
    
    def __init__(self, sequence_length: int = 10, prediction_horizon: int = 1):
        self.sequence_length = sequence_length
        self.prediction_horizon = prediction_horizon
        self.preprocessor = WSNDataPreprocessor(sequence_length, prediction_horizon)
        self.model = None
        self.trainer = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
    def prepare_data(self, data: pd.DataFrame, 
                    feature_columns: List[str],
                    train_ratio: float = 0.7,
                    val_ratio: float = 0.15,
                    batch_size: int = 64) -> Tuple[DataLoader, DataLoader, DataLoader]:
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        
        print("ğŸ”„ å‡†å¤‡LSTMè®­ç»ƒæ•°æ®...")
        
        # é¢„å¤„ç†æ•°æ®
        sequences, targets = self.preprocessor.fit_transform(data, feature_columns)
        
        print(f"   åˆ›å»ºåºåˆ—: {len(sequences)} ä¸ªæ ·æœ¬")
        print(f"   åºåˆ—å½¢çŠ¶: {sequences.shape}")
        print(f"   ç›®æ ‡å½¢çŠ¶: {targets.shape}")
        
        # æ•°æ®åˆ†å‰²
        n_samples = len(sequences)
        train_size = int(n_samples * train_ratio)
        val_size = int(n_samples * val_ratio)
        
        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        X = torch.FloatTensor(sequences)
        y = torch.FloatTensor(targets)
        
        # åˆ†å‰²æ•°æ®
        X_train, y_train = X[:train_size], y[:train_size]
        X_val, y_val = X[train_size:train_size + val_size], y[train_size:train_size + val_size]
        X_test, y_test = X[train_size + val_size:], y[train_size + val_size:]
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataset = TensorDataset(X_train, y_train)
        val_dataset = TensorDataset(X_val, y_val)
        test_dataset = TensorDataset(X_test, y_test)
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
        
        print(f"   è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
        print(f"   éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
        print(f"   æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")
        
        return train_loader, val_loader, test_loader
    
    def build_model(self, input_size: int, hidden_size: int = 64,
                   num_layers: int = 2, dropout: float = 0.2) -> LSTMPredictor:
        """æ„å»ºLSTMæ¨¡å‹"""
        
        output_size = input_size * self.prediction_horizon
        
        self.model = LSTMPredictor(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            output_size=output_size,
            dropout=dropout,
            bidirectional=False
        )
        
        self.trainer = LSTMTrainer(self.model, self.device)
        
        print(f"âœ… LSTMæ¨¡å‹æ„å»ºå®Œæˆ:")
        print(f"   è¾“å…¥ç»´åº¦: {input_size}")
        print(f"   éšè—å±‚å¤§å°: {hidden_size}")
        print(f"   å±‚æ•°: {num_layers}")
        print(f"   è¾“å‡ºç»´åº¦: {output_size}")
        print(f"   å‚æ•°æ•°é‡: {sum(p.numel() for p in self.model.parameters()):,}")
        
        return self.model
    
    def train_model(self, train_loader: DataLoader, val_loader: DataLoader,
                   epochs: int = 100, learning_rate: float = 0.001) -> Dict:
        """è®­ç»ƒæ¨¡å‹"""
        
        if self.trainer is None:
            raise ValueError("è¯·å…ˆæ„å»ºæ¨¡å‹")
        
        return self.trainer.train(train_loader, val_loader, epochs, learning_rate)
    
    def evaluate_model(self, test_loader: DataLoader) -> Dict:
        """è¯„ä¼°æ¨¡å‹"""
        
        if self.trainer is None:
            raise ValueError("è¯·å…ˆè®­ç»ƒæ¨¡å‹")
        
        return self.trainer.evaluate(test_loader)
    
    def visualize_results(self, training_stats: Dict, evaluation_metrics: Dict,
                         save_path: str = None) -> plt.Figure:
        """å¯è§†åŒ–ç»“æœ"""
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        
        # 1. è®­ç»ƒæŸå¤±æ›²çº¿
        history = training_stats['training_history']
        epochs = [h['epoch'] for h in history]
        train_losses = [h['train_loss'] for h in history]
        val_losses = [h['val_loss'] for h in history]
        
        axes[0, 0].plot(epochs, train_losses, 'b-', label='è®­ç»ƒæŸå¤±', linewidth=2)
        axes[0, 0].plot(epochs, val_losses, 'r-', label='éªŒè¯æŸå¤±', linewidth=2)
        axes[0, 0].set_title('LSTMè®­ç»ƒæŸå¤±æ›²çº¿')
        axes[0, 0].set_xlabel('è½®æ¬¡')
        axes[0, 0].set_ylabel('æŸå¤±å€¼')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. é¢„æµ‹vsçœŸå®å€¼æ•£ç‚¹å›¾
        predictions = evaluation_metrics['predictions']
        targets = evaluation_metrics['targets']
        
        # åªæ˜¾ç¤ºç¬¬ä¸€ä¸ªç‰¹å¾çš„ç»“æœ
        axes[0, 1].scatter(targets[:, 0], predictions[:, 0], alpha=0.6)
        axes[0, 1].plot([targets[:, 0].min(), targets[:, 0].max()], 
                       [targets[:, 0].min(), targets[:, 0].max()], 'r--', lw=2)
        axes[0, 1].set_title('é¢„æµ‹å€¼ vs çœŸå®å€¼')
        axes[0, 1].set_xlabel('çœŸå®å€¼')
        axes[0, 1].set_ylabel('é¢„æµ‹å€¼')
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. æ®‹å·®åˆ†å¸ƒ
        residuals = targets[:, 0] - predictions[:, 0]
        axes[0, 2].hist(residuals, bins=50, alpha=0.7, color='green')
        axes[0, 2].set_title('æ®‹å·®åˆ†å¸ƒ')
        axes[0, 2].set_xlabel('æ®‹å·®')
        axes[0, 2].set_ylabel('é¢‘æ¬¡')
        axes[0, 2].grid(True, alpha=0.3)
        
        # 4. æ—¶åºé¢„æµ‹ç¤ºä¾‹
        sample_size = min(100, len(targets))
        axes[1, 0].plot(targets[:sample_size, 0], 'b-', label='çœŸå®å€¼', linewidth=2)
        axes[1, 0].plot(predictions[:sample_size, 0], 'r--', label='é¢„æµ‹å€¼', linewidth=2)
        axes[1, 0].set_title('æ—¶åºé¢„æµ‹ç¤ºä¾‹')
        axes[1, 0].set_xlabel('æ—¶é—´æ­¥')
        axes[1, 0].set_ylabel('æ•°å€¼')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # 5. è¯„ä¼°æŒ‡æ ‡
        metrics_names = ['MAE', 'RMSE', 'RÂ²', 'MAPE(%)']
        metrics_values = [
            evaluation_metrics['mae'],
            evaluation_metrics['rmse'],
            evaluation_metrics['r2'],
            evaluation_metrics['mape']
        ]
        
        bars = axes[1, 1].bar(metrics_names, metrics_values, color='purple', alpha=0.7)
        axes[1, 1].set_title('è¯„ä¼°æŒ‡æ ‡')
        axes[1, 1].set_ylabel('æ•°å€¼')
        
        for bar, value in zip(bars, metrics_values):
            axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                           f'{value:.4f}', ha='center', va='bottom')
        
        # 6. å­¦ä¹ ç‡å˜åŒ–
        learning_rates = [h['lr'] for h in history]
        axes[1, 2].plot(epochs, learning_rates, 'g-', linewidth=2)
        axes[1, 2].set_title('å­¦ä¹ ç‡å˜åŒ–')
        axes[1, 2].set_xlabel('è½®æ¬¡')
        axes[1, 2].set_ylabel('å­¦ä¹ ç‡')
        axes[1, 2].set_yscale('log')
        axes[1, 2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"âœ… LSTMç»“æœå›¾è¡¨å·²ä¿å­˜åˆ°: {save_path}")
        
        return fig
    
    def save_model(self, save_path: str):
        """ä¿å­˜æ¨¡å‹"""
        if self.model is None:
            raise ValueError("æ²¡æœ‰å¯ä¿å­˜çš„æ¨¡å‹")
        
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'model_config': {
                'input_size': self.model.input_size,
                'hidden_size': self.model.hidden_size,
                'num_layers': self.model.num_layers,
                'output_size': self.model.output_size,
                'bidirectional': self.model.bidirectional
            },
            'preprocessor': self.preprocessor
        }, save_path)
        
        print(f"âœ… LSTMæ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
    
    def load_model(self, load_path: str):
        """åŠ è½½æ¨¡å‹"""
        checkpoint = torch.load(load_path, map_location=self.device)
        
        config = checkpoint['model_config']
        self.model = LSTMPredictor(**config)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.to(self.device)
        
        self.preprocessor = checkpoint['preprocessor']
        self.trainer = LSTMTrainer(self.model, self.device)
        
        print(f"âœ… LSTMæ¨¡å‹å·²ä» {load_path} åŠ è½½")

if __name__ == "__main__":
    # æµ‹è¯•LSTMé¢„æµ‹ç³»ç»Ÿ
    lstm_system = WSNLSTMSystem(sequence_length=10, prediction_horizon=1)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    np.random.seed(42)
    n_samples = 1000
    test_data = pd.DataFrame({
        'moteid': np.random.choice([1, 2, 3, 4, 5], n_samples),
        'temperature': 20 + 5 * np.sin(np.arange(n_samples) * 0.1) + np.random.normal(0, 1, n_samples),
        'humidity': 50 + 10 * np.cos(np.arange(n_samples) * 0.08) + np.random.normal(0, 2, n_samples),
        'light': 300 + 100 * np.sin(np.arange(n_samples) * 0.05) + np.random.normal(0, 20, n_samples),
        'voltage': 2.8 + 0.2 * np.random.random(n_samples)
    })
    
    feature_columns = ['temperature', 'humidity', 'light', 'voltage']
    
    # å‡†å¤‡æ•°æ®
    train_loader, val_loader, test_loader = lstm_system.prepare_data(
        test_data, feature_columns, batch_size=32)
    
    # æ„å»ºæ¨¡å‹
    lstm_system.build_model(input_size=len(feature_columns), hidden_size=32, num_layers=2)
    
    # è®­ç»ƒæ¨¡å‹
    training_stats = lstm_system.train_model(train_loader, val_loader, epochs=50)
    
    # è¯„ä¼°æ¨¡å‹
    evaluation_metrics = lstm_system.evaluate_model(test_loader)
    
    # å¯è§†åŒ–ç»“æœ
    lstm_system.visualize_results(training_stats, evaluation_metrics, "lstm_results.png")
    
    # ä¿å­˜æ¨¡å‹
    lstm_system.save_model("lstm_model.pth")