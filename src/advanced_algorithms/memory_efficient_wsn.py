"""
å†…å­˜ä¼˜åŒ–çš„çœŸå®æ•°æ®WSNè·¯ç”±ç³»ç»Ÿ
è§£å†³å¤§æ•°æ®é›†å†…å­˜ä¸è¶³é—®é¢˜
"""

import os
import sys
import json
import logging
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt
from pathlib import Path
from dataclasses import dataclass
from typing import Optional, Tuple, List
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(levelname)s:%(name)s:%(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

@dataclass
class MemoryEfficientConfig:
    """å†…å­˜ä¼˜åŒ–é…ç½®"""
    data_dir: str
    sequence_length: int = 10
    test_split: float = 0.2
    min_samples_per_node: int = 100
    max_samples_per_node: int = 1000  # é™åˆ¶æ¯ä¸ªèŠ‚ç‚¹çš„æœ€å¤§æ ·æœ¬æ•°
    batch_size: int = 512  # æ‰¹å¤„ç†å¤§å°
    sample_ratio: float = 0.1  # æ•°æ®é‡‡æ ·æ¯”ä¾‹
    max_nodes: int = 20  # æœ€å¤§ä½¿ç”¨èŠ‚ç‚¹æ•°

class WSNDataset(Dataset):
    """WSNæ•°æ®é›†ç±»ï¼Œæ”¯æŒæ‰¹å¤„ç†"""
    
    def __init__(self, sequences, targets):
        self.sequences = sequences
        self.targets = targets
    
    def __len__(self):
        return len(self.sequences)
    
    def __getitem__(self, idx):
        return torch.FloatTensor(self.sequences[idx]), torch.FloatTensor([self.targets[idx]])

class MemoryEfficientLSTM(nn.Module):
    """å†…å­˜ä¼˜åŒ–çš„LSTMæ¨¡å‹"""
    
    def __init__(self, input_size, hidden_size=32, output_size=1, num_layers=1):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True,
            dropout=0.1 if num_layers > 1 else 0
        )
        
        self.fc = nn.Linear(hidden_size, output_size)
        self.dropout = nn.Dropout(0.1)
    
    def forward(self, x):
        # x shape: (batch_size, seq_len, input_size)
        lstm_out, _ = self.lstm(x)
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        last_output = lstm_out[:, -1, :]
        output = self.dropout(last_output)
        output = self.fc(output)
        return output

class RealDataLoader:
    """çœŸå®æ•°æ®åŠ è½½å™¨"""
    
    def __init__(self, config: MemoryEfficientConfig):
        self.config = config
        self.sensor_data = None
        self.topology_data = None
    
    def load_intel_data(self) -> bool:
        """åŠ è½½Intel Berkeleyæ•°æ®"""
        try:
            data_file = Path(self.config.data_dir) / "data.txt"
            
            if not data_file.exists():
                logger.error(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
                return False
            
            logger.info(f"æ‰¾åˆ°æ•°æ®æ–‡ä»¶: {data_file}")
            
            # è¯»å–æ•°æ®ï¼Œä½¿ç”¨chunksizeé¿å…å†…å­˜é—®é¢˜
            chunks = []
            chunk_size = 100000
            
            for chunk in pd.read_csv(data_file, sep=' ', header=None, chunksize=chunk_size):
                chunk.columns = ['date', 'time', 'epoch', 'moteid', 'temperature', 'humidity', 'light', 'voltage']
                chunks.append(chunk)
                
                # é™åˆ¶è¯»å–çš„æ•°æ®é‡
                if len(chunks) * chunk_size > 500000:  # æœ€å¤š50ä¸‡æ¡è®°å½•
                    break
            
            self.sensor_data = pd.concat(chunks, ignore_index=True)
            logger.info(f"âœ… æˆåŠŸåŠ è½½æ•°æ®: {len(self.sensor_data)} æ¡è®°å½•")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ•°æ®å¤±è´¥: {e}")
            return False
    
    def clean_data(self):
        """æ¸…æ´—æ•°æ®"""
        if self.sensor_data is None:
            return
        
        logger.info("æ¸…æ´—æ•°æ®...")
        original_count = len(self.sensor_data)
        
        # ç§»é™¤å¼‚å¸¸å€¼
        self.sensor_data = self.sensor_data.dropna()
        
        # æ¸©åº¦èŒƒå›´è¿‡æ»¤ (åˆç†èŒƒå›´: -10 to 50Â°C)
        self.sensor_data = self.sensor_data[
            (self.sensor_data['temperature'] >= -10) & 
            (self.sensor_data['temperature'] <= 50)
        ]
        
        # æ¹¿åº¦èŒƒå›´è¿‡æ»¤ (0-100%)
        if 'humidity' in self.sensor_data.columns:
            self.sensor_data = self.sensor_data[
                (self.sensor_data['humidity'] >= 0) & 
                (self.sensor_data['humidity'] <= 100)
            ]
        
        # ç”µå‹èŒƒå›´è¿‡æ»¤ (1.8-3.3V)
        if 'voltage' in self.sensor_data.columns:
            self.sensor_data = self.sensor_data[
                (self.sensor_data['voltage'] >= 1.8) & 
                (self.sensor_data['voltage'] <= 3.3)
            ]
        
        logger.info(f"æ•°æ®æ¸…æ´—å®Œæˆ: {original_count} -> {len(self.sensor_data)} æ¡è®°å½•")

class MemoryEfficientWSNSystem:
    """å†…å­˜ä¼˜åŒ–çš„WSNç³»ç»Ÿ"""
    
    def __init__(self, config: MemoryEfficientConfig):
        self.config = config
        self.data_loader = RealDataLoader(config)
        self.model = None
        self.metrics = {
            'data_coverage': {},
            'prediction_mae': [],
            'prediction_rmse': [],
            'memory_usage': []
        }
        self.training_history = {
            'lstm_losses': [],
            'data_source': 'Intel Berkeley Lab (Memory Optimized)',
            'training_samples': 0,
            'nodes_used': 0
        }
    
    def load_and_prepare_data(self) -> bool:
        """åŠ è½½å¹¶å‡†å¤‡æ•°æ®"""
        logger.info("ğŸ” å¼€å§‹åŠ è½½çœŸå®æ•°æ®...")
        
        if not self.data_loader.load_intel_data():
            return False
        
        self.data_loader.clean_data()
        self.analyze_data_quality()
        
        return True
    
    def analyze_data_quality(self):
        """åˆ†ææ•°æ®è´¨é‡"""
        data = self.data_loader.sensor_data
        if data is None:
            return
        
        logger.info("ğŸ“Š åˆ†ææ•°æ®è´¨é‡...")
        
        total_records = len(data)
        unique_nodes = data['moteid'].nunique()
        node_counts = data['moteid'].value_counts()
        
        logger.info(f"æ€»è®°å½•æ•°: {total_records}")
        logger.info(f"èŠ‚ç‚¹æ•°: {unique_nodes}")
        logger.info(f"å¹³å‡æ¯èŠ‚ç‚¹è®°å½•æ•°: {total_records / unique_nodes:.1f}")
        logger.info(f"æ•°æ®é‡æœ€å°‘çš„èŠ‚ç‚¹: {node_counts.min()} æ¡è®°å½•")
        logger.info(f"æ•°æ®é‡æœ€å¤šçš„èŠ‚ç‚¹: {node_counts.max()} æ¡è®°å½•")
        
        sufficient_nodes = (node_counts >= self.config.min_samples_per_node).sum()
        logger.info(f"æ•°æ®å……è¶³çš„èŠ‚ç‚¹æ•° (>={self.config.min_samples_per_node}æ¡): {sufficient_nodes}")
        
        self.metrics['data_coverage'] = {
            'total_records': int(total_records),
            'unique_nodes': int(unique_nodes),
            'sufficient_nodes': int(sufficient_nodes),
            'memory_optimized': True
        }
    
    def prepare_training_data(self) -> Tuple[Optional[DataLoader], Optional[DataLoader]]:
        """å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        data = self.data_loader.sensor_data
        if data is None:
            logger.error("âŒ æ²¡æœ‰å¯ç”¨çš„ä¼ æ„Ÿå™¨æ•°æ®")
            return None, None
        
        logger.info("å‡†å¤‡LSTMè®­ç»ƒæ•°æ®ï¼ˆå†…å­˜ä¼˜åŒ–ï¼‰...")
        
        # é€‰æ‹©æ•°æ®å……è¶³çš„èŠ‚ç‚¹
        node_counts = data['moteid'].value_counts()
        valid_nodes = node_counts[node_counts >= self.config.min_samples_per_node].index
        
        # é™åˆ¶èŠ‚ç‚¹æ•°é‡
        if len(valid_nodes) > self.config.max_nodes:
            valid_nodes = valid_nodes[:self.config.max_nodes]
        
        logger.info(f"ä½¿ç”¨ {len(valid_nodes)} ä¸ªèŠ‚ç‚¹çš„æ•°æ®")
        
        sequences = []
        targets = []
        
        for node_id in valid_nodes:
            node_data = data[data['moteid'] == node_id].copy()
            node_data = node_data.sort_values('epoch')
            
            # é™åˆ¶æ¯ä¸ªèŠ‚ç‚¹çš„æ ·æœ¬æ•°
            if len(node_data) > self.config.max_samples_per_node:
                # å‡åŒ€é‡‡æ ·
                indices = np.linspace(0, len(node_data)-1, self.config.max_samples_per_node, dtype=int)
                node_data = node_data.iloc[indices]
            
            # æå–ç‰¹å¾
            features = ['temperature', 'humidity', 'light', 'voltage']
            available_features = [f for f in features if f in node_data.columns]
            
            if len(available_features) < 2:
                continue
            
            feature_data = node_data[available_features].values
            
            # æ•°æ®æ ‡å‡†åŒ–
            feature_data = (feature_data - np.mean(feature_data, axis=0)) / (np.std(feature_data, axis=0) + 1e-8)
            
            # åˆ›å»ºåºåˆ—ï¼ˆé‡‡æ ·ï¼‰
            max_sequences = int(len(feature_data) * self.config.sample_ratio)
            step_size = max(1, (len(feature_data) - self.config.sequence_length) // max_sequences)
            
            for i in range(0, len(feature_data) - self.config.sequence_length, step_size):
                seq = feature_data[i:i + self.config.sequence_length]
                target = feature_data[i + self.config.sequence_length, 0]  # é¢„æµ‹æ¸©åº¦
                
                sequences.append(seq)
                targets.append(target)
        
        if len(sequences) == 0:
            logger.error("âŒ æ— æ³•åˆ›å»ºè®­ç»ƒåºåˆ—")
            return None, None
        
        logger.info(f"âœ… åˆ›å»ºäº† {len(sequences)} ä¸ªè®­ç»ƒåºåˆ—")
        logger.info(f"   åºåˆ—é•¿åº¦: {self.config.sequence_length}")
        logger.info(f"   ç‰¹å¾æ•°: {len(available_features)}")
        
        # æ•°æ®åˆ†å‰²
        split_idx = int(len(sequences) * (1 - self.config.test_split))
        
        train_sequences = sequences[:split_idx]
        train_targets = targets[:split_idx]
        test_sequences = sequences[split_idx:]
        test_targets = targets[split_idx:]
        
        # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
        train_dataset = WSNDataset(train_sequences, train_targets)
        test_dataset = WSNDataset(test_sequences, test_targets)
        
        train_loader = DataLoader(train_dataset, batch_size=self.config.batch_size, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=self.config.batch_size, shuffle=False)
        
        self.training_history['training_samples'] = len(sequences)
        self.training_history['nodes_used'] = len(valid_nodes)
        
        return train_loader, test_loader
    
    def train_model(self, epochs=50) -> bool:
        """è®­ç»ƒæ¨¡å‹"""
        logger.info("ğŸš€ å¼€å§‹è®­ç»ƒLSTMæ¨¡å‹...")
        
        train_loader, test_loader = self.prepare_training_data()
        if train_loader is None or test_loader is None:
            logger.error("âŒ æ— æ³•å‡†å¤‡è®­ç»ƒæ•°æ®")
            return False
        
        # è·å–è¾“å…¥ç»´åº¦
        sample_batch = next(iter(train_loader))
        input_size = sample_batch[0].shape[2]
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = MemoryEfficientLSTM(
            input_size=input_size,
            hidden_size=32,
            output_size=1
        )
        
        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)
        criterion = nn.MSELoss()
        
        logger.info(f"è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
        logger.info(f"æµ‹è¯•æ‰¹æ¬¡æ•°: {len(test_loader)}")
        
        # è®­ç»ƒå¾ªç¯
        self.model.train()
        for epoch in range(epochs):
            epoch_loss = 0.0
            batch_count = 0
            
            for batch_x, batch_y in train_loader:
                optimizer.zero_grad()
                
                predictions = self.model(batch_x).squeeze()
                loss = criterion(predictions, batch_y.squeeze())
                
                loss.backward()
                optimizer.step()
                
                epoch_loss += loss.item()
                batch_count += 1
            
            avg_loss = epoch_loss / batch_count
            self.training_history['lstm_losses'].append(avg_loss)
            
            if epoch % 10 == 0:
                logger.info(f"Epoch {epoch}/{epochs}, Loss: {avg_loss:.4f}")
        
        # æµ‹è¯•è¯„ä¼°
        self.model.eval()
        test_mae = 0.0
        test_rmse = 0.0
        test_batches = 0
        
        with torch.no_grad():
            for batch_x, batch_y in test_loader:
                predictions = self.model(batch_x).squeeze()
                
                mae = torch.mean(torch.abs(predictions - batch_y.squeeze())).item()
                rmse = torch.sqrt(torch.mean((predictions - batch_y.squeeze()) ** 2)).item()
                
                test_mae += mae
                test_rmse += rmse
                test_batches += 1
        
        test_mae /= test_batches
        test_rmse /= test_batches
        
        self.metrics['prediction_mae'].append(test_mae)
        self.metrics['prediction_rmse'].append(test_rmse)
        
        logger.info(f"âœ… æµ‹è¯•ç»“æœ - MAE: {test_mae:.4f}, RMSE: {test_rmse:.4f}")
        
        return True
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        results_dir = Path(__file__).parent.parent.parent / "results" / "memory_efficient_wsn"
        results_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æŒ‡æ ‡
        with open(results_dir / "memory_efficient_metrics.json", 'w', encoding='utf-8') as f:
            json.dump(self.metrics, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜è®­ç»ƒå†å²
        with open(results_dir / "memory_efficient_training_history.json", 'w', encoding='utf-8') as f:
            json.dump(self.training_history, f, indent=2)
        
        # ä¿å­˜æ¨¡å‹
        if self.model:
            torch.save(self.model.state_dict(), results_dir / "memory_efficient_lstm_model.pth")
        
        logger.info(f"âœ… ç»“æœä¿å­˜åˆ°: {results_dir}")
    
    def visualize_results(self):
        """å¯è§†åŒ–ç»“æœ"""
        results_dir = Path(__file__).parent.parent.parent / "results" / "memory_efficient_wsn"
        results_dir.mkdir(parents=True, exist_ok=True)
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # LSTMè®­ç»ƒæŸå¤±
        if self.training_history['lstm_losses']:
            axes[0, 0].plot(self.training_history['lstm_losses'])
            axes[0, 0].set_title('LSTMè®­ç»ƒæŸå¤± (å†…å­˜ä¼˜åŒ–)')
            axes[0, 0].set_xlabel('Epoch')
            axes[0, 0].set_ylabel('MSE Loss')
        
        # æ•°æ®ä½¿ç”¨æƒ…å†µ
        if self.metrics['data_coverage']:
            coverage = self.metrics['data_coverage']
            labels = ['æ€»èŠ‚ç‚¹', 'ä½¿ç”¨èŠ‚ç‚¹']
            values = [coverage['unique_nodes'], self.training_history['nodes_used']]
            axes[0, 1].bar(labels, values)
            axes[0, 1].set_title('èŠ‚ç‚¹ä½¿ç”¨æƒ…å†µ')
            axes[0, 1].set_ylabel('èŠ‚ç‚¹æ•°')
        
        # é¢„æµ‹æ€§èƒ½
        if self.metrics['prediction_mae']:
            axes[1, 0].bar(['MAE', 'RMSE'], 
                          [self.metrics['prediction_mae'][0], self.metrics['prediction_rmse'][0]])
            axes[1, 0].set_title('é¢„æµ‹æ€§èƒ½ (å†…å­˜ä¼˜åŒ–)')
            axes[1, 0].set_ylabel('è¯¯å·®')
        
        # è®­ç»ƒæ ·æœ¬ç»Ÿè®¡
        axes[1, 1].bar(['è®­ç»ƒæ ·æœ¬'], [self.training_history['training_samples']])
        axes[1, 1].set_title('è®­ç»ƒæ ·æœ¬æ•°é‡')
        axes[1, 1].set_ylabel('æ ·æœ¬æ•°')
        
        plt.suptitle('å†…å­˜ä¼˜åŒ–çš„çœŸå®æ•°æ®WSNåˆ†æ', fontsize=14)
        plt.tight_layout()
        plt.savefig(results_dir / 'memory_efficient_results.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info("âœ… å¯è§†åŒ–å®Œæˆ")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¯åŠ¨å†…å­˜ä¼˜åŒ–çš„çœŸå®æ•°æ®WSNç³»ç»Ÿ")
    
    # é…ç½®
    config = MemoryEfficientConfig(
        data_dir=str(Path(__file__).parent.parent.parent / "data")
    )
    
    # åˆ›å»ºç³»ç»Ÿ
    wsn_system = MemoryEfficientWSNSystem(config)
    
    # åŠ è½½æ•°æ®
    if not wsn_system.load_and_prepare_data():
        logger.error("âŒ æ— æ³•åŠ è½½æ•°æ®ï¼Œç¨‹åºé€€å‡º")
        return
    
    # è®­ç»ƒæ¨¡å‹
    if wsn_system.train_model(epochs=30):
        logger.info("âœ… æ¨¡å‹è®­ç»ƒæˆåŠŸ")
    else:
        logger.error("âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥")
        return
    
    # ä¿å­˜å’Œå¯è§†åŒ–ç»“æœ
    wsn_system.save_results()
    wsn_system.visualize_results()
    
    logger.info("âœ… å†…å­˜ä¼˜åŒ–WSNç³»ç»Ÿè¿è¡Œå®Œæˆ")
    logger.info("ğŸ“Š ä½¿ç”¨çœŸå®Intel Berkeleyæ•°æ®é›†ï¼ˆå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬ï¼‰")
    logger.info("ğŸ”§ ä¼˜åŒ–æªæ–½ï¼šæ•°æ®é‡‡æ ·ã€æ‰¹å¤„ç†ã€èŠ‚ç‚¹é™åˆ¶")

if __name__ == "__main__":
    main()