"""
çœŸæ­£ä½¿ç”¨Intel BerkeleyçœŸå®æ•°æ®é›†çš„WSNè·¯ç”±ç³»ç»Ÿ
ä¸å†æœ‰ä»»ä½•è™šå‡å®£ä¼ 
"""

import sys
import os
from pathlib import Path
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import networkx as nx
import logging
from typing import Dict, List, Tuple, Optional
import json
from dataclasses import dataclass
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class RealDataConfig:
    """çœŸå®æ•°æ®é…ç½®"""
    data_dir: str = "data"
    min_samples_per_node: int = 100  # æ¯ä¸ªèŠ‚ç‚¹æœ€å°‘æ ·æœ¬æ•°
    sequence_length: int = 10        # LSTMåºåˆ—é•¿åº¦
    test_split: float = 0.2          # æµ‹è¯•é›†æ¯”ä¾‹

class RealIntelDataLoader:
    """çœŸæ­£çš„Intel Berkeleyæ•°æ®åŠ è½½å™¨"""
    
    def __init__(self, data_dir: str):
        self.data_dir = Path(data_dir)
        self.sensor_data = None
        self.topology_data = None
        self.connectivity_data = None
        
    def load_intel_berkeley_data(self) -> Optional[pd.DataFrame]:
        """åŠ è½½çœŸå®çš„Intel Berkeleyæ•°æ®"""
        logger.info("å°è¯•åŠ è½½çœŸå®Intel Berkeleyæ•°æ®...")
        
        # å°è¯•å¤šä¸ªå¯èƒ½çš„æ•°æ®æ–‡ä»¶ä½ç½®
        possible_paths = [
            self.data_dir / "data.txt",
            self.data_dir / "intel_berkeley" / "data.txt",
            self.data_dir / "real_datasets" / "intel_berkeley" / "data.txt",
            self.data_dir / "processed" / "cleaned_data.csv"
        ]
        
        for data_path in possible_paths:
            if data_path.exists():
                logger.info(f"æ‰¾åˆ°æ•°æ®æ–‡ä»¶: {data_path}")
                try:
                    if data_path.suffix == '.csv':
                        data = pd.read_csv(data_path)
                    else:
                        # Intel BerkeleyåŸå§‹æ ¼å¼
                        # æ ¼å¼: date time epoch moteid temperature humidity light voltage
                        data = pd.read_csv(data_path, sep=r'\s+', header=None,
                                         names=['date', 'time', 'epoch', 'moteid', 
                                               'temperature', 'humidity', 'light', 'voltage'])
                    
                    logger.info(f"âœ… æˆåŠŸåŠ è½½æ•°æ®: {len(data)} æ¡è®°å½•")
                    logger.info(f"   èŠ‚ç‚¹æ•°: {data['moteid'].nunique()}")
                    logger.info(f"   æ—¶é—´èŒƒå›´: {data['epoch'].min()} - {data['epoch'].max()}")
                    
                    # åŸºæœ¬æ•°æ®æ¸…æ´—
                    data = self.clean_data(data)
                    self.sensor_data = data
                    return data
                    
                except Exception as e:
                    logger.warning(f"åŠ è½½ {data_path} å¤±è´¥: {e}")
                    continue
        
        logger.error("âŒ æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨çš„Intel Berkeleyæ•°æ®æ–‡ä»¶")
        logger.error("è¯·ç¡®ä¿æ•°æ®æ–‡ä»¶å­˜åœ¨äºä»¥ä¸‹ä½ç½®ä¹‹ä¸€:")
        for path in possible_paths:
            logger.error(f"  - {path}")
        
        return None
    
    def clean_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—çœŸå®æ•°æ®"""
        logger.info("æ¸…æ´—æ•°æ®...")
        
        original_len = len(data)
        
        # ç§»é™¤ç¼ºå¤±å€¼
        data = data.dropna()
        
        # ç§»é™¤æ˜æ˜¾å¼‚å¸¸å€¼
        for col in ['temperature', 'humidity', 'light', 'voltage']:
            if col in data.columns:
                Q1 = data[col].quantile(0.01)
                Q3 = data[col].quantile(0.99)
                data = data[(data[col] >= Q1) & (data[col] <= Q3)]
        
        # ç¡®ä¿èŠ‚ç‚¹IDæ˜¯æ•´æ•°
        if 'moteid' in data.columns:
            data['moteid'] = data['moteid'].astype(int)
        
        logger.info(f"æ•°æ®æ¸…æ´—å®Œæˆ: {original_len} -> {len(data)} æ¡è®°å½•")
        return data
    
    def load_topology_data(self) -> Optional[pd.DataFrame]:
        """åŠ è½½æ‹“æ‰‘æ•°æ®"""
        topology_paths = [
            self.data_dir / "topology.txt",
            self.data_dir / "mote_locs.txt",
            self.data_dir / "processed" / "topology.csv"
        ]
        
        for path in topology_paths:
            if path.exists():
                try:
                    if path.suffix == '.csv':
                        data = pd.read_csv(path)
                    else:
                        data = pd.read_csv(path, sep=r'\s+', header=None,
                                         names=['moteid', 'x', 'y'])
                    
                    logger.info(f"âœ… åŠ è½½æ‹“æ‰‘æ•°æ®: {len(data)} ä¸ªèŠ‚ç‚¹")
                    self.topology_data = data
                    return data
                except Exception as e:
                    logger.warning(f"åŠ è½½æ‹“æ‰‘æ–‡ä»¶ {path} å¤±è´¥: {e}")
        
        logger.warning("âš ï¸ æœªæ‰¾åˆ°æ‹“æ‰‘æ•°æ®ï¼Œå°†ä½¿ç”¨ä¼ æ„Ÿå™¨æ•°æ®æ¨æ–­")
        return None

class RealLSTMPredictor(nn.Module):
    """åŸºäºçœŸå®æ•°æ®çš„LSTMé¢„æµ‹å™¨"""
    
    def __init__(self, input_size: int, hidden_size: int, output_size: int):
        super().__init__()
        self.hidden_size = hidden_size
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, dropout=0.1)
        self.fc = nn.Linear(hidden_size, output_size)
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        output = self.dropout(lstm_out[:, -1, :])
        output = self.fc(output)
        return output

class RealWSNSystem:
    """åŸºäºçœŸå®æ•°æ®çš„WSNç³»ç»Ÿ"""
    
    def __init__(self, config: RealDataConfig):
        self.config = config
        self.data_loader = RealIntelDataLoader(config.data_dir)
        self.sensor_data = None
        self.topology_data = None
        self.network_graph = None
        
        # æ¨¡å‹ç»„ä»¶
        self.lstm_predictor = None
        
        # è®­ç»ƒå†å²
        self.training_history = {
            'lstm_losses': [],
            'data_source': 'unknown',
            'training_samples': 0,
            'nodes_used': 0
        }
        
        # æ€§èƒ½æŒ‡æ ‡
        self.metrics = {
            'prediction_mae': [],
            'prediction_rmse': [],
            'data_coverage': {},
            'note': 'åŸºäºçœŸå®Intel Berkeleyæ•°æ®é›†'
        }
    
    def load_real_data(self) -> bool:
        """åŠ è½½çœŸå®æ•°æ®"""
        logger.info("ğŸ” å¼€å§‹åŠ è½½çœŸå®æ•°æ®...")
        
        # åŠ è½½ä¼ æ„Ÿå™¨æ•°æ®
        self.sensor_data = self.data_loader.load_intel_berkeley_data()
        if self.sensor_data is None:
            logger.error("âŒ æ— æ³•åŠ è½½ä¼ æ„Ÿå™¨æ•°æ®")
            return False
        
        # åŠ è½½æ‹“æ‰‘æ•°æ®
        self.topology_data = self.data_loader.load_topology_data()
        
        # æ•°æ®ç»Ÿè®¡
        self.analyze_data_quality()
        
        return True
    
    def analyze_data_quality(self):
        """åˆ†ææ•°æ®è´¨é‡"""
        if self.sensor_data is None:
            return
        
        logger.info("ğŸ“Š åˆ†ææ•°æ®è´¨é‡...")
        
        # åŸºæœ¬ç»Ÿè®¡
        total_records = len(self.sensor_data)
        unique_nodes = self.sensor_data['moteid'].nunique()
        
        # æ¯ä¸ªèŠ‚ç‚¹çš„æ•°æ®é‡
        node_counts = self.sensor_data['moteid'].value_counts()
        
        logger.info(f"æ€»è®°å½•æ•°: {total_records}")
        logger.info(f"èŠ‚ç‚¹æ•°: {unique_nodes}")
        logger.info(f"å¹³å‡æ¯èŠ‚ç‚¹è®°å½•æ•°: {total_records / unique_nodes:.1f}")
        logger.info(f"æ•°æ®é‡æœ€å°‘çš„èŠ‚ç‚¹: {node_counts.min()} æ¡è®°å½•")
        logger.info(f"æ•°æ®é‡æœ€å¤šçš„èŠ‚ç‚¹: {node_counts.max()} æ¡è®°å½•")
        
        # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        sufficient_nodes = (node_counts >= self.config.min_samples_per_node).sum()
        logger.info(f"æ•°æ®å……è¶³çš„èŠ‚ç‚¹æ•° (>={self.config.min_samples_per_node}æ¡): {sufficient_nodes}")
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        self.metrics['data_coverage'] = {
            'total_records': int(total_records),
            'unique_nodes': int(unique_nodes),
            'sufficient_nodes': int(sufficient_nodes),
            'min_records_per_node': int(node_counts.min()),
            'max_records_per_node': int(node_counts.max()),
            'avg_records_per_node': float(total_records / unique_nodes)
        }
        
        self.training_history['data_source'] = 'Intel Berkeley Lab (Real)'
        self.training_history['nodes_used'] = int(sufficient_nodes)
    
    def prepare_lstm_training_data(self):
        """å‡†å¤‡LSTMè®­ç»ƒæ•°æ®"""
        if self.sensor_data is None:
            logger.error("âŒ æ²¡æœ‰å¯ç”¨çš„ä¼ æ„Ÿå™¨æ•°æ®")
            return None, None
        
        logger.info("å‡†å¤‡LSTMè®­ç»ƒæ•°æ®...")
        
        # é€‰æ‹©æ•°æ®å……è¶³çš„èŠ‚ç‚¹
        node_counts = self.sensor_data['moteid'].value_counts()
        valid_nodes = node_counts[node_counts >= self.config.min_samples_per_node].index
        
        if len(valid_nodes) == 0:
            logger.error("âŒ æ²¡æœ‰è¶³å¤Ÿæ•°æ®çš„èŠ‚ç‚¹")
            return None, None
        
        logger.info(f"ä½¿ç”¨ {len(valid_nodes)} ä¸ªèŠ‚ç‚¹çš„æ•°æ®")
        
        sequences = []
        targets = []
        
        for node_id in valid_nodes:
            node_data = self.sensor_data[self.sensor_data['moteid'] == node_id].copy()
            node_data = node_data.sort_values('epoch')
            
            # æå–ç‰¹å¾
            features = ['temperature', 'humidity', 'light', 'voltage']
            available_features = [f for f in features if f in node_data.columns]
            
            if len(available_features) < 2:
                continue
            
            feature_data = node_data[available_features].values
            
            # åˆ›å»ºåºåˆ—
            for i in range(len(feature_data) - self.config.sequence_length):
                seq = feature_data[i:i + self.config.sequence_length]
                target = feature_data[i + self.config.sequence_length, 0]  # é¢„æµ‹æ¸©åº¦
                
                sequences.append(seq)
                targets.append(target)
        
        if len(sequences) == 0:
            logger.error("âŒ æ— æ³•åˆ›å»ºè®­ç»ƒåºåˆ—")
            return None, None
        
        X = torch.FloatTensor(sequences)
        y = torch.FloatTensor(targets)
        
        logger.info(f"âœ… åˆ›å»ºäº† {len(sequences)} ä¸ªè®­ç»ƒåºåˆ—")
        logger.info(f"   ç‰¹å¾ç»´åº¦: {X.shape}")
        logger.info(f"   ç›®æ ‡ç»´åº¦: {y.shape}")
        
        self.training_history['training_samples'] = len(sequences)
        
        return X, y
    
    def train_lstm_on_real_data(self, epochs=50):
        """ä½¿ç”¨çœŸå®æ•°æ®è®­ç»ƒLSTM"""
        logger.info("ğŸš€ å¼€å§‹ä½¿ç”¨çœŸå®æ•°æ®è®­ç»ƒLSTM...")
        
        X, y = self.prepare_lstm_training_data()
        if X is None or y is None:
            logger.error("âŒ æ— æ³•å‡†å¤‡è®­ç»ƒæ•°æ®")
            return False
        
        # æ•°æ®åˆ†å‰²
        split_idx = int(len(X) * (1 - self.config.test_split))
        X_train, X_test = X[:split_idx], X[split_idx:]
        y_train, y_test = y[:split_idx], y[split_idx:]
        
        # åˆå§‹åŒ–æ¨¡å‹
        input_size = X.shape[2]
        self.lstm_predictor = RealLSTMPredictor(
            input_size=input_size,
            hidden_size=32,
            output_size=1
        )
        
        optimizer = torch.optim.Adam(self.lstm_predictor.parameters(), lr=0.001)
        criterion = nn.MSELoss()
        
        logger.info(f"è®­ç»ƒé›†å¤§å°: {len(X_train)}")
        logger.info(f"æµ‹è¯•é›†å¤§å°: {len(X_test)}")
        
        # è®­ç»ƒå¾ªç¯
        self.lstm_predictor.train()
        for epoch in range(epochs):
            optimizer.zero_grad()
            
            predictions = self.lstm_predictor(X_train).squeeze()
            loss = criterion(predictions, y_train)
            
            loss.backward()
            optimizer.step()
            
            self.training_history['lstm_losses'].append(loss.item())
            
            if epoch % 10 == 0:
                logger.info(f"Epoch {epoch}/{epochs}, Loss: {loss.item():.4f}")
        
        # æµ‹è¯•è¯„ä¼°
        self.lstm_predictor.eval()
        with torch.no_grad():
            test_predictions = self.lstm_predictor(X_test).squeeze()
            test_mae = torch.mean(torch.abs(test_predictions - y_test)).item()
            test_rmse = torch.sqrt(torch.mean((test_predictions - y_test) ** 2)).item()
            
            self.metrics['prediction_mae'].append(test_mae)
            self.metrics['prediction_rmse'].append(test_rmse)
            
            logger.info(f"âœ… æµ‹è¯•ç»“æœ - MAE: {test_mae:.4f}, RMSE: {test_rmse:.4f}")
        
        return True
    
    def save_real_results(self):
        """ä¿å­˜çœŸå®ç»“æœ"""
        results_dir = Path(__file__).parent.parent.parent / "results" / "real_data_wsn"
        results_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æŒ‡æ ‡
        with open(results_dir / "real_metrics.json", 'w', encoding='utf-8') as f:
            json.dump(self.metrics, f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜è®­ç»ƒå†å²
        with open(results_dir / "real_training_history.json", 'w', encoding='utf-8') as f:
            json.dump(self.training_history, f, indent=2)
        
        # ä¿å­˜æ¨¡å‹
        if self.lstm_predictor:
            torch.save(self.lstm_predictor.state_dict(), results_dir / "real_lstm_model.pth")
        
        logger.info(f"âœ… ç»“æœä¿å­˜åˆ°: {results_dir}")
    
    def visualize_real_results(self):
        """å¯è§†åŒ–çœŸå®ç»“æœ"""
        results_dir = Path(__file__).parent.parent.parent / "results" / "real_data_wsn"
        results_dir.mkdir(parents=True, exist_ok=True)
        
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # LSTMè®­ç»ƒæŸå¤±
        if self.training_history['lstm_losses']:
            axes[0, 0].plot(self.training_history['lstm_losses'])
            axes[0, 0].set_title('LSTMè®­ç»ƒæŸå¤± (çœŸå®æ•°æ®)')
            axes[0, 0].set_xlabel('Epoch')
            axes[0, 0].set_ylabel('MSE Loss')
        
        # æ•°æ®è¦†ç›–æƒ…å†µ
        if self.metrics['data_coverage']:
            coverage = self.metrics['data_coverage']
            labels = ['æ€»èŠ‚ç‚¹', 'æ•°æ®å……è¶³èŠ‚ç‚¹']
            values = [coverage['unique_nodes'], coverage['sufficient_nodes']]
            axes[0, 1].bar(labels, values)
            axes[0, 1].set_title('æ•°æ®è¦†ç›–æƒ…å†µ')
            axes[0, 1].set_ylabel('èŠ‚ç‚¹æ•°')
        
        # é¢„æµ‹æ€§èƒ½
        if self.metrics['prediction_mae']:
            axes[1, 0].bar(['MAE', 'RMSE'], 
                          [self.metrics['prediction_mae'][0], self.metrics['prediction_rmse'][0]])
            axes[1, 0].set_title('é¢„æµ‹æ€§èƒ½ (çœŸå®æ•°æ®æµ‹è¯•)')
            axes[1, 0].set_ylabel('è¯¯å·®')
        
        # æ•°æ®ç»Ÿè®¡
        if self.sensor_data is not None:
            temp_data = self.sensor_data['temperature'].dropna()
            axes[1, 1].hist(temp_data, bins=30, alpha=0.7)
            axes[1, 1].set_title('æ¸©åº¦æ•°æ®åˆ†å¸ƒ (çœŸå®æ•°æ®)')
            axes[1, 1].set_xlabel('æ¸©åº¦ (Â°C)')
            axes[1, 1].set_ylabel('é¢‘æ¬¡')
        
        plt.suptitle('åŸºäºçœŸå®Intel Berkeleyæ•°æ®çš„WSNåˆ†æ', fontsize=14)
        plt.tight_layout()
        plt.savefig(results_dir / 'real_data_results.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info("âœ… å¯è§†åŒ–å®Œæˆ")

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¯åŠ¨åŸºäºçœŸå®æ•°æ®çš„WSNç³»ç»Ÿ")
    
    # é…ç½®
    config = RealDataConfig(
        data_dir=str(Path(__file__).parent.parent.parent / "data")
    )
    
    # åˆ›å»ºç³»ç»Ÿ
    wsn_system = RealWSNSystem(config)
    
    # åŠ è½½çœŸå®æ•°æ®
    if not wsn_system.load_real_data():
        logger.error("âŒ æ— æ³•åŠ è½½çœŸå®æ•°æ®ï¼Œç¨‹åºé€€å‡º")
        return
    
    # è®­ç»ƒæ¨¡å‹
    if wsn_system.train_lstm_on_real_data(epochs=50):
        logger.info("âœ… æ¨¡å‹è®­ç»ƒæˆåŠŸ")
    else:
        logger.error("âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥")
        return
    
    # ä¿å­˜å’Œå¯è§†åŒ–ç»“æœ
    wsn_system.save_real_results()
    wsn_system.visualize_real_results()
    
    logger.info("âœ… åŸºäºçœŸå®æ•°æ®çš„WSNç³»ç»Ÿè¿è¡Œå®Œæˆ")
    logger.info("ğŸ“Š è¿™æ¬¡ä½¿ç”¨çš„æ˜¯çœŸå®çš„Intel Berkeleyæ•°æ®é›†")

if __name__ == "__main__":
    main()