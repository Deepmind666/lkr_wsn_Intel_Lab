"""
GNN-CTO: Graph Neural Network-based Chain Topology Optimization
åŸºäºå›¾ç¥ç»ç½‘ç»œçš„é“¾å¼æ‹“æ‰‘ä¼˜åŒ–ç®—æ³•

æ ¸å¿ƒåˆ›æ–°ï¼š
1. å°†WSNå»ºæ¨¡ä¸ºåŠ¨æ€å›¾ç»“æ„
2. ä½¿ç”¨å›¾æ³¨æ„åŠ›ç½‘ç»œ(GAT)å­¦ä¹ èŠ‚ç‚¹ç‰¹å¾
3. é“¾å¼æ‹“æ‰‘ä¼˜åŒ–ä¸èƒ½é‡å‡è¡¡
4. è‡ªé€‚åº”æ‹“æ‰‘é‡æ„æœºåˆ¶

ä½œè€…: WSNç ”ç©¶å›¢é˜Ÿ
æ—¥æœŸ: 2025å¹´1æœˆ
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import networkx as nx
from typing import Dict, List, Tuple, Optional, Union
import json
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GATConv, global_mean_pool
from torch_geometric.data import Data, Batch
from sklearn.preprocessing import StandardScaler
from dataclasses import dataclass
import warnings
warnings.filterwarnings('ignore')

@dataclass
class NodeFeatures:
    """èŠ‚ç‚¹ç‰¹å¾è¡¨ç¤º"""
    node_id: int
    position: np.ndarray      # [x, y] ä½ç½®åæ ‡
    energy: float            # å‰©ä½™èƒ½é‡
    degree: int              # èŠ‚ç‚¹åº¦
    centrality: float        # ä¸­å¿ƒæ€§
    cluster_id: int          # æ‰€å±ç°‡ID
    is_cluster_head: bool    # æ˜¯å¦ä¸ºç°‡å¤´
    distance_to_bs: float    # åˆ°åŸºç«™è·ç¦»
    neighbor_energy_avg: float  # é‚»å±…å¹³å‡èƒ½é‡
    
    def to_vector(self) -> np.ndarray:
        """è½¬æ¢ä¸ºç‰¹å¾å‘é‡"""
        return np.array([
            self.position[0], self.position[1],
            self.energy, self.degree, self.centrality,
            float(self.is_cluster_head), self.distance_to_bs,
            self.neighbor_energy_avg
        ])

@dataclass
class ChainTopology:
    """é“¾å¼æ‹“æ‰‘ç»“æ„"""
    chain_id: int
    nodes: List[int]          # é“¾ä¸­çš„èŠ‚ç‚¹åºåˆ—
    head_node: int           # é“¾å¤´èŠ‚ç‚¹
    energy_cost: float       # é“¾çš„æ€»èƒ½è€—
    reliability: float       # é“¾çš„å¯é æ€§
    length: int              # é“¾é•¿åº¦
    
    def get_chain_efficiency(self) -> float:
        """è®¡ç®—é“¾æ•ˆç‡"""
        if self.energy_cost == 0:
            return 0
        return self.reliability / (self.energy_cost * self.length)

class GraphAttentionNetwork(nn.Module):
    """å›¾æ³¨æ„åŠ›ç½‘ç»œæ¨¡å‹"""
    
    def __init__(self, input_dim: int = 8, hidden_dim: int = 64, 
                 output_dim: int = 32, num_heads: int = 4, dropout: float = 0.1):
        super(GraphAttentionNetwork, self).__init__()
        
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        self.num_heads = num_heads
        
        # ç¬¬ä¸€å±‚GAT
        self.gat1 = GATConv(
            in_channels=input_dim,
            out_channels=hidden_dim // num_heads,
            heads=num_heads,
            dropout=dropout,
            concat=True
        )
        
        # ç¬¬äºŒå±‚GAT
        self.gat2 = GATConv(
            in_channels=hidden_dim,
            out_channels=output_dim,
            heads=1,
            dropout=dropout,
            concat=False
        )
        
        # æ‰¹å½’ä¸€åŒ–
        self.bn1 = nn.BatchNorm1d(hidden_dim)
        self.bn2 = nn.BatchNorm1d(output_dim)
        
        # Dropout
        self.dropout = nn.Dropout(dropout)
        
        # è¾“å‡ºå±‚
        self.classifier = nn.Sequential(
            nn.Linear(output_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, 3)  # 3ç±»ï¼šæ™®é€šèŠ‚ç‚¹ã€é“¾å¤´ã€ä¸­ç»§èŠ‚ç‚¹
        )
        
        # å›å½’å±‚ï¼ˆé¢„æµ‹èƒ½è€—ï¼‰
        self.regressor = nn.Sequential(
            nn.Linear(output_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, 1)
        )
    
    def forward(self, x: torch.Tensor, edge_index: torch.Tensor, 
                batch: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """å‰å‘ä¼ æ’­"""
        # ç¬¬ä¸€å±‚GAT
        x = self.gat1(x, edge_index)
        x = self.bn1(x)
        x = F.relu(x)
        x = self.dropout(x)
        
        # ç¬¬äºŒå±‚GAT
        node_embeddings = self.gat2(x, edge_index)
        node_embeddings = self.bn2(node_embeddings)
        node_embeddings = F.relu(node_embeddings)
        
        # åˆ†ç±»è¾“å‡ºï¼ˆèŠ‚ç‚¹è§’è‰²ï¼‰
        node_roles = self.classifier(node_embeddings)
        
        # å›å½’è¾“å‡ºï¼ˆèƒ½è€—é¢„æµ‹ï¼‰
        energy_pred = self.regressor(node_embeddings)
        
        return node_embeddings, node_roles, energy_pred

class ChainOptimizer:
    """é“¾å¼æ‹“æ‰‘ä¼˜åŒ–å™¨"""
    
    def __init__(self, max_chain_length: int = 5, energy_threshold: float = 0.1):
        self.max_chain_length = max_chain_length
        self.energy_threshold = energy_threshold
    
    def construct_chains(self, nodes_features: List[NodeFeatures], 
                        adjacency_matrix: np.ndarray) -> List[ChainTopology]:
        """æ„å»ºé“¾å¼æ‹“æ‰‘"""
        num_nodes = len(nodes_features)
        visited = set()
        chains = []
        chain_id = 0
        
        # æŒ‰èƒ½é‡æ’åºï¼Œä¼˜å…ˆé€‰æ‹©é«˜èƒ½é‡èŠ‚ç‚¹ä½œä¸ºé“¾å¤´
        sorted_nodes = sorted(
            enumerate(nodes_features), 
            key=lambda x: x[1].energy, 
            reverse=True
        )
        
        for node_idx, node_feature in sorted_nodes:
            if node_idx in visited:
                continue
            
            # æ„å»ºä»¥å½“å‰èŠ‚ç‚¹ä¸ºèµ·ç‚¹çš„é“¾
            chain = self._build_chain_from_node(
                start_node=node_idx,
                nodes_features=nodes_features,
                adjacency_matrix=adjacency_matrix,
                visited=visited
            )
            
            if len(chain) > 1:  # è‡³å°‘åŒ…å«2ä¸ªèŠ‚ç‚¹
                chain_topology = self._create_chain_topology(
                    chain_id=chain_id,
                    chain_nodes=chain,
                    nodes_features=nodes_features
                )
                chains.append(chain_topology)
                chain_id += 1
                
                # æ ‡è®°å·²è®¿é—®çš„èŠ‚ç‚¹
                visited.update(chain)
        
        return chains
    
    def _build_chain_from_node(self, start_node: int, nodes_features: List[NodeFeatures],
                              adjacency_matrix: np.ndarray, visited: set) -> List[int]:
        """ä»æŒ‡å®šèŠ‚ç‚¹å¼€å§‹æ„å»ºé“¾"""
        chain = [start_node]
        current_node = start_node
        
        while len(chain) < self.max_chain_length:
            # æ‰¾åˆ°æœ€ä½³ä¸‹ä¸€ä¸ªèŠ‚ç‚¹
            next_node = self._find_best_next_node(
                current_node=current_node,
                chain=chain,
                nodes_features=nodes_features,
                adjacency_matrix=adjacency_matrix,
                visited=visited
            )
            
            if next_node is None:
                break
            
            chain.append(next_node)
            current_node = next_node
        
        return chain
    
    def _find_best_next_node(self, current_node: int, chain: List[int],
                           nodes_features: List[NodeFeatures],
                           adjacency_matrix: np.ndarray, visited: set) -> Optional[int]:
        """æ‰¾åˆ°æœ€ä½³çš„ä¸‹ä¸€ä¸ªèŠ‚ç‚¹"""
        candidates = []
        
        # éå†å½“å‰èŠ‚ç‚¹çš„é‚»å±…
        for neighbor in range(len(adjacency_matrix)):
            if (adjacency_matrix[current_node][neighbor] > 0 and 
                neighbor not in visited and 
                neighbor not in chain and
                nodes_features[neighbor].energy > self.energy_threshold):
                
                # è®¡ç®—å€™é€‰èŠ‚ç‚¹çš„è¯„åˆ†
                score = self._calculate_node_score(
                    node_idx=neighbor,
                    current_chain=chain,
                    nodes_features=nodes_features
                )
                candidates.append((neighbor, score))
        
        if not candidates:
            return None
        
        # é€‰æ‹©è¯„åˆ†æœ€é«˜çš„èŠ‚ç‚¹
        candidates.sort(key=lambda x: x[1], reverse=True)
        return candidates[0][0]
    
    def _calculate_node_score(self, node_idx: int, current_chain: List[int],
                            nodes_features: List[NodeFeatures]) -> float:
        """è®¡ç®—èŠ‚ç‚¹è¯„åˆ†"""
        node = nodes_features[node_idx]
        
        # èƒ½é‡æƒé‡
        energy_score = node.energy * 0.4
        
        # è·ç¦»æƒé‡ï¼ˆè·ç¦»åŸºç«™è¶Šè¿‘è¶Šå¥½ï¼‰
        distance_score = (1.0 / (1.0 + node.distance_to_bs)) * 0.3
        
        # åº¦æƒé‡ï¼ˆé€‚ä¸­çš„åº¦æœ€å¥½ï¼‰
        optimal_degree = 3
        degree_score = 1.0 / (1.0 + abs(node.degree - optimal_degree)) * 0.2
        
        # ä¸­å¿ƒæ€§æƒé‡
        centrality_score = node.centrality * 0.1
        
        return energy_score + distance_score + degree_score + centrality_score
    
    def _create_chain_topology(self, chain_id: int, chain_nodes: List[int],
                             nodes_features: List[NodeFeatures]) -> ChainTopology:
        """åˆ›å»ºé“¾æ‹“æ‰‘å¯¹è±¡"""
        head_node = chain_nodes[0]
        
        # è®¡ç®—é“¾çš„æ€»èƒ½è€—
        total_energy_cost = 0
        for i, node_idx in enumerate(chain_nodes):
            node = nodes_features[node_idx]
            # é“¾å¤´é¢å¤–èƒ½è€—
            if i == 0:
                total_energy_cost += 0.01
            # ä¼ è¾“èƒ½è€—ï¼ˆè·ç¦»ç›¸å…³ï¼‰
            if i < len(chain_nodes) - 1:
                next_node = nodes_features[chain_nodes[i + 1]]
                distance = np.linalg.norm(node.position - next_node.position)
                total_energy_cost += 0.001 * (distance ** 2)
        
        # è®¡ç®—é“¾çš„å¯é æ€§ï¼ˆåŸºäºèŠ‚ç‚¹èƒ½é‡ï¼‰
        min_energy = min(nodes_features[idx].energy for idx in chain_nodes)
        avg_energy = np.mean([nodes_features[idx].energy for idx in chain_nodes])
        reliability = (min_energy * 0.6 + avg_energy * 0.4)
        
        return ChainTopology(
            chain_id=chain_id,
            nodes=chain_nodes,
            head_node=head_node,
            energy_cost=total_energy_cost,
            reliability=reliability,
            length=len(chain_nodes)
        )

class GNNCTOAlgorithm:
    """GNN-CTOä¸»ç®—æ³•ç±»"""
    
    def __init__(self, num_nodes: int, area: Tuple[int, int], rounds: int, 
                 transmission_range: float = 50, initial_energy: float = 1.0, 
                 packet_size: int = 2000, E_elec: float = 50e-9, 
                 E_amp: float = 100e-12, max_chain_length: int = 6, 
                 gnn_hidden_dim: int = 128, gnn_output_dim: int = 64, 
                 gnn_heads: int = 4, gnn_epochs: int = 50, 
                 learning_rate: float = 0.005, topology_update_interval: int = 10):
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        self.num_nodes = num_nodes
        self.area = area
        self.rounds = rounds
        self.transmission_range = transmission_range
        self.initial_energy = initial_energy
        self.packet_size = packet_size
        self.E_elec = E_elec
        self.E_amp = E_amp
        self.max_chain_length = max_chain_length
        self.topology_update_interval = topology_update_interval
        
        # åˆå§‹åŒ–GNNæ¨¡å‹
        self.gnn_model = GraphAttentionNetwork(
            hidden_dim=gnn_hidden_dim,
            output_dim=gnn_output_dim,
            num_heads=gnn_heads
        ).to(self.device)
        
        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.Adam(
            self.gnn_model.parameters(), 
            lr=learning_rate
        )
        
        # æŸå¤±å‡½æ•°
        self.criterion_classification = nn.CrossEntropyLoss()
        self.criterion_regression = nn.MSELoss()
        
        # é“¾ä¼˜åŒ–å™¨
        self.chain_optimizer = ChainOptimizer(
            max_chain_length=max_chain_length
        )
        
        # ç‰¹å¾æ ‡å‡†åŒ–
        self.feature_scaler = StandardScaler()
        
        # ä»¿çœŸçŠ¶æ€
        self.network_graph: Optional[nx.Graph] = None
        self.nodes_features: List[NodeFeatures] = []
        self.chains: List[ChainTopology] = []
        self.performance_metrics: Dict[str, List] = {
            'energy_efficiency': [],
            'network_lifetime': [],
            'routing_success_rate': [],
            'average_latency': [],
            'throughput': [],
            'dead_nodes': []
        }
        self.current_round = 0

        self.training_history: Dict[str, List] = {
            'losses': [],
            'classification_losses': [],
            'regression_losses': [],
            'accuracies': []
        }
    
    def extract_node_features(self, nodes_data: np.ndarray, 
                            base_station_pos: np.ndarray) -> List[NodeFeatures]:
        """æå–èŠ‚ç‚¹ç‰¹å¾"""
        num_nodes = len(nodes_data)
        node_features = []
        
        # æ„å»ºé‚»æ¥çŸ©é˜µ
        adjacency_matrix = self._build_adjacency_matrix(nodes_data)
        
        # è®¡ç®—ç½‘ç»œå›¾
        G = nx.from_numpy_array(adjacency_matrix)
        
        # è®¡ç®—ä¸­å¿ƒæ€§
        try:
            centrality = nx.betweenness_centrality(G)
        except:
            centrality = {i: 0.0 for i in range(num_nodes)}
        
        for i, node_data in enumerate(nodes_data):
            if node_data[3] <= 0:  # æ­»äº¡èŠ‚ç‚¹
                continue
            
            # è®¡ç®—é‚»å±…å¹³å‡èƒ½é‡
            neighbors = np.where(adjacency_matrix[i] > 0)[0]
            neighbor_energies = [nodes_data[j][2] for j in neighbors if nodes_data[j][3] > 0]
            neighbor_energy_avg = np.mean(neighbor_energies) if neighbor_energies else 0
            
            # è®¡ç®—åˆ°åŸºç«™è·ç¦»
            distance_to_bs = np.linalg.norm(node_data[:2] - base_station_pos)
            
            node_feature = NodeFeatures(
                node_id=i,
                position=node_data[:2],
                energy=node_data[2],
                degree=len(neighbors),
                centrality=centrality.get(i, 0.0),
                cluster_id=-1,  # å¾…åˆ†é…
                is_cluster_head=False,  # å¾…ç¡®å®š
                distance_to_bs=distance_to_bs,
                neighbor_energy_avg=neighbor_energy_avg
            )
            
            node_features.append(node_feature)
        
        return node_features
    
    def _build_adjacency_matrix(self, nodes_data: np.ndarray) -> np.ndarray:
        """æ„å»ºé‚»æ¥çŸ©é˜µ"""
        num_nodes = len(nodes_data)
        adjacency_matrix = np.zeros((num_nodes, num_nodes))
        
        for i in range(num_nodes):
            if nodes_data[i][3] <= 0:  # æ­»äº¡èŠ‚ç‚¹
                continue
            for j in range(i + 1, num_nodes):
                if nodes_data[j][3] <= 0:  # æ­»äº¡èŠ‚ç‚¹
                    continue
                
                distance = np.linalg.norm(nodes_data[i][:2] - nodes_data[j][:2])
                if distance <= self.transmission_range:
                    adjacency_matrix[i][j] = 1
                    adjacency_matrix[j][i] = 1
        
        return adjacency_matrix
    
    def create_graph_data(self, node_features: List[NodeFeatures],
                         adjacency_matrix: np.ndarray) -> Data:
        """åˆ›å»ºPyTorch Geometricå›¾æ•°æ®"""
        # èŠ‚ç‚¹ç‰¹å¾çŸ©é˜µ
        node_feature_matrix = np.array([nf.to_vector() for nf in node_features])
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        if hasattr(self.feature_scaler, 'mean_'):
            node_feature_matrix = self.feature_scaler.transform(node_feature_matrix)
        else:
            node_feature_matrix = self.feature_scaler.fit_transform(node_feature_matrix)
        
        # è¾¹ç´¢å¼•
        edge_indices = np.where(adjacency_matrix > 0)
        edge_index = torch.tensor(np.vstack(edge_indices), dtype=torch.long)
        
        # è½¬æ¢ä¸ºå¼ é‡
        x = torch.tensor(node_feature_matrix, dtype=torch.float)
        
        return Data(x=x, edge_index=edge_index)
    
    def generate_training_labels(self, node_features: List[NodeFeatures],
                               chains: List[ChainTopology]) -> Tuple[torch.Tensor, torch.Tensor]:
        """ç”Ÿæˆè®­ç»ƒæ ‡ç­¾"""
        num_nodes = len(node_features)
        
        # èŠ‚ç‚¹è§’è‰²æ ‡ç­¾ (0: æ™®é€šèŠ‚ç‚¹, 1: é“¾å¤´, 2: ä¸­ç»§èŠ‚ç‚¹)
        role_labels = torch.zeros(num_nodes, dtype=torch.long)
        
        # èƒ½è€—æ ‡ç­¾ï¼ˆæ¨¡æ‹ŸçœŸå®èƒ½è€—ï¼‰
        energy_labels = torch.zeros(num_nodes, dtype=torch.float)
        
        node_id_to_idx = {nf.node_id: i for i, nf in enumerate(node_features)}
        
        for chain in chains:
            for i, node_id in enumerate(chain.nodes):
                if node_id in node_id_to_idx:
                    idx = node_id_to_idx[node_id]
                    
                    if i == 0:  # é“¾å¤´
                        role_labels[idx] = 1
                        energy_labels[idx] = 0.01  # é“¾å¤´åŸºç¡€èƒ½è€—
                    else:  # ä¸­ç»§èŠ‚ç‚¹
                        role_labels[idx] = 2
                        energy_labels[idx] = 0.005  # ä¸­ç»§èŠ‚ç‚¹èƒ½è€—
                    
                    # æ·»åŠ ä¼ è¾“èƒ½è€—
                    if i < len(chain.nodes) - 1:
                        next_node_id = chain.nodes[i + 1]
                        if next_node_id in node_id_to_idx:
                            current_pos = node_features[idx].position
                            next_idx = node_id_to_idx[next_node_id]
                            next_pos = node_features[next_idx].position
                            distance = np.linalg.norm(current_pos - next_pos)
                            energy_labels[idx] += 0.001 * (distance ** 2)
        
        return role_labels, energy_labels
    
    def train_gnn(self, training_data: List[Tuple[Data, torch.Tensor, torch.Tensor]],
                  epochs: int = 100) -> Dict:
        """è®­ç»ƒGNNæ¨¡å‹"""
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒGNNæ¨¡å‹ï¼Œå…±{epochs}ä¸ªepochs")
        
        self.gnn_model.train()
        
        for epoch in range(epochs):
            total_loss = 0
            total_classification_loss = 0
            total_regression_loss = 0
            correct_predictions = 0
            total_predictions = 0
            
            for graph_data, role_labels, energy_labels in training_data:
                graph_data = graph_data.to(self.device)
                role_labels = role_labels.to(self.device)
                energy_labels = energy_labels.to(self.device)
                
                self.optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                node_embeddings, role_pred, energy_pred = self.gnn_model(
                    graph_data.x, graph_data.edge_index
                )
                
                # è®¡ç®—æŸå¤±
                classification_loss = self.criterion_classification(role_pred, role_labels)
                regression_loss = self.criterion_regression(energy_pred.squeeze(), energy_labels)
                
                # æ€»æŸå¤±ï¼ˆåŠ æƒç»„åˆï¼‰
                loss = classification_loss + 0.5 * regression_loss
                
                # åå‘ä¼ æ’­
                loss.backward()
                self.optimizer.step()
                
                # ç»Ÿè®¡
                total_loss += loss.item()
                total_classification_loss += classification_loss.item()
                total_regression_loss += regression_loss.item()
                
                # è®¡ç®—å‡†ç¡®ç‡
                _, predicted = torch.max(role_pred.data, 1)
                total_predictions += role_labels.size(0)
                correct_predictions += (predicted == role_labels).sum().item()
            
            # è®°å½•è®­ç»ƒå†å²
            avg_loss = total_loss / len(training_data)
            avg_classification_loss = total_classification_loss / len(training_data)
            avg_regression_loss = total_regression_loss / len(training_data)
            accuracy = correct_predictions / total_predictions
            
            self.training_history['losses'].append(avg_loss)
            self.training_history['classification_losses'].append(avg_classification_loss)
            self.training_history['regression_losses'].append(avg_regression_loss)
            self.training_history['accuracies'].append(accuracy)
            
            # è¿›åº¦æŠ¥å‘Š
            if (epoch + 1) % 20 == 0:
                print(f"Epoch {epoch + 1}/{epochs}: "
                      f"Loss={avg_loss:.4f}, "
                      f"Classification Loss={avg_classification_loss:.4f}, "
                      f"Regression Loss={avg_regression_loss:.4f}, "
                      f"Accuracy={accuracy:.3f}")
        
        print("âœ… GNNæ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        return self.training_history
    
    def optimize_topology(self, nodes_data: np.ndarray, 
                         base_station_pos: np.ndarray) -> Tuple[List[ChainTopology], Dict]:
        """ä¼˜åŒ–ç½‘ç»œæ‹“æ‰‘"""
        # æå–èŠ‚ç‚¹ç‰¹å¾
        node_features = self.extract_node_features(nodes_data, base_station_pos)
        
        if len(node_features) == 0:
            return [], {'error': 'No alive nodes'}
        
        # æ„å»ºé‚»æ¥çŸ©é˜µ
        adjacency_matrix = self._build_adjacency_matrix(nodes_data)
        
        # åˆ›å»ºå›¾æ•°æ®
        graph_data = self.create_graph_data(node_features, adjacency_matrix)
        
        # GNNé¢„æµ‹
        self.gnn_model.eval()
        with torch.no_grad():
            graph_data = graph_data.to(self.device)
            node_embeddings, role_pred, energy_pred = self.gnn_model(
                graph_data.x, graph_data.edge_index
            )
            
            # è·å–é¢„æµ‹ç»“æœ
            predicted_roles = torch.argmax(role_pred, dim=1).cpu().numpy()
            predicted_energies = energy_pred.squeeze().cpu().numpy()
        
        # æ›´æ–°èŠ‚ç‚¹ç‰¹å¾ï¼ˆåŸºäºGNNé¢„æµ‹ï¼‰
        for i, node_feature in enumerate(node_features):
            if predicted_roles[i] == 1:  # é“¾å¤´
                node_feature.is_cluster_head = True
        
        # æ„å»ºä¼˜åŒ–çš„é“¾å¼æ‹“æ‰‘
        optimized_chains = self.chain_optimizer.construct_chains(
            node_features, adjacency_matrix
        )
        
        # è®¡ç®—æ‹“æ‰‘è´¨é‡æŒ‡æ ‡
        topology_metrics = self._calculate_topology_metrics(
            optimized_chains, node_features, predicted_energies
        )
        
        return optimized_chains, topology_metrics
    
    def _calculate_topology_metrics(self, chains: List[ChainTopology],
                                  node_features: List[NodeFeatures],
                                  predicted_energies: np.ndarray) -> Dict:
        """è®¡ç®—æ‹“æ‰‘è´¨é‡æŒ‡æ ‡"""
        if not chains:
            return {
                'chain_count': 0,
                'average_chain_length': 0,
                'total_energy_cost': 0,
                'average_reliability': 0,
                'topology_efficiency': 0,
                'coverage_ratio': 0
            }
        
        # åŸºæœ¬ç»Ÿè®¡
        chain_count = len(chains)
        chain_lengths = [chain.length for chain in chains]
        average_chain_length = np.mean(chain_lengths)
        
        # èƒ½è€—ç»Ÿè®¡
        total_energy_cost = sum(chain.energy_cost for chain in chains)
        
        # å¯é æ€§ç»Ÿè®¡
        reliabilities = [chain.reliability for chain in chains]
        average_reliability = np.mean(reliabilities)
        
        # æ‹“æ‰‘æ•ˆç‡
        efficiencies = [chain.get_chain_efficiency() for chain in chains]
        topology_efficiency = np.mean(efficiencies) if efficiencies else 0
        
        # è¦†ç›–ç‡ï¼ˆé“¾ä¸­èŠ‚ç‚¹å æ€»å­˜æ´»èŠ‚ç‚¹çš„æ¯”ä¾‹ï¼‰
        nodes_in_chains = set()
        for chain in chains:
            nodes_in_chains.update(chain.nodes)
        coverage_ratio = len(nodes_in_chains) / len(node_features) if node_features else 0
        
        return {
            'chain_count': chain_count,
            'average_chain_length': average_chain_length,
            'total_energy_cost': total_energy_cost,
            'average_reliability': average_reliability,
            'topology_efficiency': topology_efficiency,
            'coverage_ratio': coverage_ratio,
            'chain_details': [
                {
                    'chain_id': chain.chain_id,
                    'nodes': chain.nodes,
                    'head_node': chain.head_node,
                    'length': chain.length,
                    'energy_cost': chain.energy_cost,
                    'reliability': chain.reliability,
                    'efficiency': chain.get_chain_efficiency()
                }
                for chain in chains
            ]
        }
    
    def simulate_network_operation(self, initial_nodes_data: np.ndarray,
                                 base_station_pos: np.ndarray,
                                 max_rounds: int = 200) -> Dict:
        """æ¨¡æ‹Ÿç½‘ç»œè¿è¡Œ"""
        print("ğŸ” å¼€å§‹GNN-CTOç½‘ç»œè¿è¡Œæ¨¡æ‹Ÿ...")
        
        simulation_results = {
            'network_lifetime': 0,
            'total_energy_consumption': 0,
            'topology_evolution': [],
            'performance_metrics': []
        }
        
        nodes_data = initial_nodes_data.copy()
        initial_energy = np.sum(nodes_data[:, 2])
        
        for round_num in range(max_rounds):
            # ä¼˜åŒ–æ‹“æ‰‘
            chains, topology_metrics = self.optimize_topology(nodes_data, base_station_pos)
            
            if not chains:
                simulation_results['network_lifetime'] = round_num
                break
            
            # æ¨¡æ‹Ÿèƒ½é‡æ¶ˆè€—
            energy_consumption = self._simulate_energy_consumption(
                nodes_data, chains, base_station_pos
            )
            
            # æ›´æ–°èŠ‚ç‚¹çŠ¶æ€
            nodes_data[:, 2] -= energy_consumption
            nodes_data[nodes_data[:, 2] <= 0, 3] = 0  # æ ‡è®°æ­»äº¡èŠ‚ç‚¹
            
            # è®°å½•è½®æ¬¡ä¿¡æ¯
            alive_count = np.sum(nodes_data[:, 3] > 0)
            remaining_energy = np.sum(nodes_data[:, 2])
            
            round_info = {
                'round': round_num + 1,
                'alive_nodes': alive_count,
                'remaining_energy': remaining_energy,
                'chain_count': len(chains),
                'topology_metrics': topology_metrics
            }
            
            simulation_results['topology_evolution'].append(round_info)
            
            # æ£€æŸ¥ç½‘ç»œæ­»äº¡æ¡ä»¶
            if alive_count <= len(initial_nodes_data) * 0.1:  # 90%èŠ‚ç‚¹æ­»äº¡
                simulation_results['network_lifetime'] = round_num + 1
                break
        
        # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
        final_energy = np.sum(nodes_data[:, 2])
        simulation_results['total_energy_consumption'] = initial_energy - final_energy
        
        # è®¡ç®—å¹³å‡æ€§èƒ½æŒ‡æ ‡
        if simulation_results['topology_evolution']:
            avg_metrics = {}
            for key in ['chain_count', 'topology_metrics']:
                if key == 'topology_metrics':
                    # è®¡ç®—æ‹“æ‰‘æŒ‡æ ‡çš„å¹³å‡å€¼
                    metric_keys = ['topology_efficiency', 'average_reliability', 'coverage_ratio']
                    for metric_key in metric_keys:
                        values = [round_info[key].get(metric_key, 0) 
                                for round_info in simulation_results['topology_evolution']]
                        avg_metrics[f'avg_{metric_key}'] = np.mean(values)
                else:
                    values = [round_info[key] for round_info in simulation_results['topology_evolution']]
                    avg_metrics[f'avg_{key}'] = np.mean(values)
            
            simulation_results['performance_metrics'] = avg_metrics
        
        print(f"âœ… æ¨¡æ‹Ÿå®Œæˆï¼ç½‘ç»œå¯¿å‘½: {simulation_results['network_lifetime']} è½®")
        return simulation_results
    
    def _simulate_energy_consumption(self, nodes_data: np.ndarray,
                                   chains: List[ChainTopology],
                                   base_station_pos: np.ndarray) -> np.ndarray:
        """æ¨¡æ‹Ÿèƒ½é‡æ¶ˆè€—"""
        energy_consumption = np.zeros(len(nodes_data))
        
        # åŸºç¡€æ„ŸçŸ¥èƒ½è€—
        base_sensing_energy = 0.001
        energy_consumption[nodes_data[:, 3] > 0] += base_sensing_energy
        
        # é“¾å¼ä¼ è¾“èƒ½è€—
        for chain in chains:
            for i, node_id in enumerate(chain.nodes):
                if node_id < len(nodes_data) and nodes_data[node_id][3] > 0:
                    # é“¾å¤´é¢å¤–èƒ½è€—
                    if i == 0:
                        energy_consumption[node_id] += 0.01
                    
                    # ä¼ è¾“èƒ½è€—
                    if i < len(chain.nodes) - 1:
                        next_node_id = chain.nodes[i + 1]
                        if next_node_id < len(nodes_data):
                            current_pos = nodes_data[node_id][:2]
                            next_pos = nodes_data[next_node_id][:2]
                            distance = np.linalg.norm(current_pos - next_pos)
                            energy_consumption[node_id] += 0.001 * (distance ** 2)
                    else:
                        # æœ€åä¸€ä¸ªèŠ‚ç‚¹ä¼ è¾“åˆ°åŸºç«™
                        distance = np.linalg.norm(nodes_data[node_id][:2] - base_station_pos)
                        energy_consumption[node_id] += 0.001 * (distance ** 2)
        
        return energy_consumption
    
    def save_model(self, filepath: str):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        model_data = {
            'model_state_dict': self.gnn_model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'feature_scaler': {
                'mean_': self.feature_scaler.mean_.tolist() if hasattr(self.feature_scaler, 'mean_') else None,
                'scale_': self.feature_scaler.scale_.tolist() if hasattr(self.feature_scaler, 'scale_') else None
            },
            'training_history': self.training_history,
            'performance_history': self.performance_history,
            'hyperparameters': {
                'communication_range': self.communication_range,
                'learning_rate': self.learning_rate,
                'max_chain_length': self.chain_optimizer.max_chain_length,
                'energy_threshold': self.chain_optimizer.energy_threshold
            }
        }
        
        torch.save(model_data, filepath)
        print(f"âœ… GNN-CTOæ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")
    
    def load_model(self, filepath: str):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        model_data = torch.load(filepath, map_location=self.device)
        
        # æ¢å¤æ¨¡å‹çŠ¶æ€
        self.gnn_model.load_state_dict(model_data['model_state_dict'])
        self.optimizer.load_state_dict(model_data['optimizer_state_dict'])
        
        # æ¢å¤ç‰¹å¾æ ‡å‡†åŒ–å™¨
        if model_data['feature_scaler']['mean_'] is not None:
            self.feature_scaler.mean_ = np.array(model_data['feature_scaler']['mean_'])
            self.feature_scaler.scale_ = np.array(model_data['feature_scaler']['scale_'])
        
        # æ¢å¤å†å²è®°å½•
        self.training_history = model_data['training_history']
        self.performance_history = model_data['performance_history']
        
        print(f"âœ… GNN-CTOæ¨¡å‹å·²ä» {filepath} åŠ è½½")

def demonstrate_gnn_cto():
    """æ¼”ç¤ºGNN-CTOç®—æ³•"""
    print("ğŸ¯ GNN-CTOç®—æ³•æ¼”ç¤º")
    print("=" * 60)
    
    # ç”Ÿæˆæ¨¡æ‹Ÿç½‘ç»œæ•°æ®
    np.random.seed(42)
    num_nodes = 54
    
    # èŠ‚ç‚¹ä½ç½®ï¼ˆéšæœºåˆ†å¸ƒåœ¨50x50åŒºåŸŸï¼‰
    positions = np.random.uniform(0, 50, (num_nodes, 2))
    
    # åˆå§‹èƒ½é‡ï¼ˆå½’ä¸€åŒ–åˆ°0-1ï¼‰
    initial_energy = np.ones(num_nodes)
    
    # å­˜æ´»çŠ¶æ€ï¼ˆ1ä¸ºå­˜æ´»ï¼Œ0ä¸ºæ­»äº¡ï¼‰
    alive_status = np.ones(num_nodes)
    
    # ç»„åˆèŠ‚ç‚¹æ•°æ® [x, y, energy, alive]
    nodes_data = np.column_stack([positions, initial_energy, alive_status])
    
    # åŸºç«™ä½ç½®
    base_station_pos = np.array([25.0, 25.0])
    
    # åˆ›å»ºGNN-CTOç®—æ³•å®ä¾‹
    gnn_cto = GNNCTOAlgorithm(communication_range=15.0, learning_rate=0.001)
    
    # ç”Ÿæˆè®­ç»ƒæ•°æ®
    print("ğŸ“Š ç”Ÿæˆè®­ç»ƒæ•°æ®...")
    training_data = []
    
    for _ in range(10):  # ç”Ÿæˆ10ä¸ªè®­ç»ƒæ ·æœ¬
        # éšæœºæ‰°åŠ¨èŠ‚ç‚¹èƒ½é‡
        perturbed_data = nodes_data.copy()
        perturbed_data[:, 2] += np.random.normal(0, 0.1, num_nodes)
        perturbed_data[:, 2] = np.clip(perturbed_data[:, 2], 0, 1)
        
        # æå–ç‰¹å¾
        node_features = gnn_cto.extract_node_features(perturbed_data, base_station_pos)
        adjacency_matrix = gnn_cto._build_adjacency_matrix(perturbed_data)
        
        # æ„å»ºé“¾ï¼ˆç”¨äºç”Ÿæˆæ ‡ç­¾ï¼‰
        chains = gnn_cto.chain_optimizer.construct_chains(node_features, adjacency_matrix)
        
        # åˆ›å»ºå›¾æ•°æ®
        graph_data = gnn_cto.create_graph_data(node_features, adjacency_matrix)
        
        # ç”Ÿæˆæ ‡ç­¾
        role_labels, energy_labels = gnn_cto.generate_training_labels(node_features, chains)
        
        training_data.append((graph_data, role_labels, energy_labels))
    
    # è®­ç»ƒGNNæ¨¡å‹
    training_history = gnn_cto.train_gnn(training_data, num_epochs=50)
    
    # è¿è¡Œç½‘ç»œæ¨¡æ‹Ÿ
    simulation_results = gnn_cto.simulate_network_operation(
        initial_nodes_data=nodes_data,
        base_station_pos=base_station_pos,
        max_rounds=200
    )
    
    # ä¿å­˜ç»“æœ
    results_dir = "results/gnn_cto"
    import os
    os.makedirs(results_dir, exist_ok=True)
    
    # ä¿å­˜æ¨¡å‹
    gnn_cto.save_model(f"{results_dir}/gnn_cto_model.pth")
    
    # ä¿å­˜æ¨¡æ‹Ÿç»“æœ
    with open(f"{results_dir}/simulation_results.json", 'w') as f:
        json.dump(simulation_results, f, indent=2)
    
    # æ‰“å°ç»“æœæ‘˜è¦
    print("\nğŸ“Š GNN-CTOç®—æ³•æ€§èƒ½æ‘˜è¦")
    print("=" * 60)
    print(f"ğŸ”‹ ç½‘ç»œå¯¿å‘½: {simulation_results['network_lifetime']} è½®")
    print(f"âš¡ æ€»èƒ½è€—: {simulation_results['total_energy_consumption']:.3f}")
    
    if simulation_results['performance_metrics']:
        metrics = simulation_results['performance_metrics']
        print(f"ğŸ”— å¹³å‡é“¾æ•°: {metrics.get('avg_chain_count', 0):.1f}")
        print(f"ğŸ“ˆ å¹³å‡æ‹“æ‰‘æ•ˆç‡: {metrics.get('avg_topology_efficiency', 0):.3f}")
        print(f"ğŸ¯ å¹³å‡å¯é æ€§: {metrics.get('avg_average_reliability', 0):.3f}")
        print(f"ğŸ“Š å¹³å‡è¦†ç›–ç‡: {metrics.get('avg_coverage_ratio', 0):.3f}")
    
    print(f"ğŸ§  GNNè®­ç»ƒå‡†ç¡®ç‡: {training_history['accuracies'][-1]:.3f}")
    print(f"ğŸ“‰ æœ€ç»ˆè®­ç»ƒæŸå¤±: {training_history['losses'][-1]:.4f}")
    
    return gnn_cto, training_history, simulation_results

if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    gnn_cto, training_history, simulation_results = demonstrate_gnn_cto()