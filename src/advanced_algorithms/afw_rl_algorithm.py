"""
AFW-RL: Adaptive Fuzzy Weight Reinforcement Learning Algorithm
è‡ªé€‚åº”æ¨¡ç³Šé€»è¾‘æƒé‡å¼ºåŒ–å­¦ä¹ ç®—æ³•

æ ¸å¿ƒåˆ›æ–°ï¼š
1. å°†é™æ€æ¨¡ç³Šé€»è¾‘æƒé‡è½¬åŒ–ä¸ºåŠ¨æ€å­¦ä¹ ç­–ç•¥
2. Q-Learningä¸æ¨¡ç³Šé€»è¾‘çš„æ·±åº¦èåˆ
3. å®æ—¶ç½‘ç»œçŠ¶æ€æ„ŸçŸ¥å’Œæƒé‡è‡ªé€‚åº”è°ƒæ•´

ä½œè€…: WSNç ”ç©¶å›¢é˜Ÿ
æ—¥æœŸ: 2025å¹´1æœˆ
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from typing import Dict, List, Tuple, Optional
import json
import pickle
from dataclasses import dataclass
from collections import defaultdict, deque
import warnings
warnings.filterwarnings('ignore')

@dataclass
class NetworkState:
    """ç½‘ç»œçŠ¶æ€è¡¨ç¤º"""
    remaining_energy_ratio: float  # å‰©ä½™èƒ½é‡æ¯”ä¾‹
    alive_nodes_ratio: float      # å­˜æ´»èŠ‚ç‚¹æ¯”ä¾‹
    network_connectivity: float   # ç½‘ç»œè¿é€šæ€§
    energy_variance: float        # èƒ½é‡æ–¹å·®ï¼ˆå‡è¡¡æ€§ï¼‰
    avg_node_degree: float        # å¹³å‡èŠ‚ç‚¹åº¦
    
    def to_vector(self) -> np.ndarray:
        """è½¬æ¢ä¸ºçŠ¶æ€å‘é‡"""
        return np.array([
            self.remaining_energy_ratio,
            self.alive_nodes_ratio,
            self.network_connectivity,
            self.energy_variance,
            self.avg_node_degree
        ])
    
    def discretize(self, bins: int = 5) -> int:
        """çŠ¶æ€ç¦»æ•£åŒ–"""
        vector = self.to_vector()
        # å°†è¿ç»­çŠ¶æ€æ˜ å°„åˆ°ç¦»æ•£çŠ¶æ€ç©ºé—´
        discrete_values = []
        for val in vector:
            discrete_val = min(int(val * bins), bins - 1)
            discrete_values.append(discrete_val)
        
        # ç»„åˆæˆå•ä¸€çŠ¶æ€ID
        state_id = 0
        for i, val in enumerate(discrete_values):
            state_id += val * (bins ** i)
        
        return state_id

@dataclass
class FuzzyWeightAction:
    """æ¨¡ç³Šæƒé‡åŠ¨ä½œ"""
    energy_weight: float     # èƒ½é‡æƒé‡
    location_weight: float   # ä½ç½®æƒé‡
    connectivity_weight: float  # è¿é€šæ€§æƒé‡
    
    def __post_init__(self):
        """ç¡®ä¿æƒé‡å’Œä¸º1"""
        total = self.energy_weight + self.location_weight + self.connectivity_weight
        if total > 0:
            self.energy_weight /= total
            self.location_weight /= total
            self.connectivity_weight /= total
    
    def to_vector(self) -> np.ndarray:
        """è½¬æ¢ä¸ºå‘é‡"""
        return np.array([self.energy_weight, self.location_weight, self.connectivity_weight])

class FuzzyLogicClusterHead:
    """æ”¹è¿›çš„æ¨¡ç³Šé€»è¾‘ç°‡å¤´é€‰æ‹©"""
    
    def __init__(self, weights: FuzzyWeightAction):
        self.weights = weights
    
    def fuzzy_membership_energy(self, energy_ratio: float) -> float:
        """èƒ½é‡æ¨¡ç³Šéš¶å±åº¦å‡½æ•° - æ”¹è¿›ç‰ˆ"""
        if energy_ratio >= 0.8:
            return 1.0
        elif energy_ratio >= 0.6:
            return 0.8 + 0.2 * (energy_ratio - 0.6) / 0.2
        elif energy_ratio >= 0.4:
            return 0.5 + 0.3 * (energy_ratio - 0.4) / 0.2
        elif energy_ratio >= 0.2:
            return 0.2 + 0.3 * (energy_ratio - 0.2) / 0.2
        else:
            return 0.1 * energy_ratio / 0.2
    
    def fuzzy_membership_location(self, distance_ratio: float) -> float:
        """ä½ç½®æ¨¡ç³Šéš¶å±åº¦å‡½æ•° - æ”¹è¿›ç‰ˆ"""
        # è·ç¦»åŸºç«™è¶Šè¿‘è¶Šå¥½
        if distance_ratio <= 0.2:
            return 1.0
        elif distance_ratio <= 0.4:
            return 0.8 + 0.2 * (0.4 - distance_ratio) / 0.2
        elif distance_ratio <= 0.6:
            return 0.5 + 0.3 * (0.6 - distance_ratio) / 0.2
        elif distance_ratio <= 0.8:
            return 0.2 + 0.3 * (0.8 - distance_ratio) / 0.2
        else:
            return 0.1 * (1.0 - distance_ratio) / 0.2
    
    def fuzzy_membership_connectivity(self, neighbor_count: int, max_neighbors: int = 10) -> float:
        """è¿é€šæ€§æ¨¡ç³Šéš¶å±åº¦å‡½æ•° - æ”¹è¿›ç‰ˆ"""
        ratio = min(neighbor_count / max_neighbors, 1.0)
        if ratio >= 0.8:
            return 1.0
        elif ratio >= 0.6:
            return 0.8 + 0.2 * (ratio - 0.6) / 0.2
        elif ratio >= 0.4:
            return 0.5 + 0.3 * (ratio - 0.4) / 0.2
        elif ratio >= 0.2:
            return 0.2 + 0.3 * (ratio - 0.2) / 0.2
        else:
            return 0.1 * ratio / 0.2
    
    def calculate_fuzzy_score(self, energy_ratio: float, distance_ratio: float, 
                            neighbor_count: int) -> float:
        """è®¡ç®—æ¨¡ç³Šç»¼åˆè¯„åˆ†"""
        energy_score = self.fuzzy_membership_energy(energy_ratio)
        location_score = self.fuzzy_membership_location(distance_ratio)
        connectivity_score = self.fuzzy_membership_connectivity(neighbor_count)
        
        # ä½¿ç”¨åŠ¨æ€æƒé‡
        fuzzy_score = (self.weights.energy_weight * energy_score + 
                      self.weights.location_weight * location_score + 
                      self.weights.connectivity_weight * connectivity_score)
        
        return fuzzy_score

class QLearningAgent:
    """Q-Learningæ™ºèƒ½ä½“"""
    
    def __init__(self, state_size: int, action_size: int, 
                 learning_rate: float = 0.1, discount_factor: float = 0.95,
                 epsilon: float = 1.0, epsilon_decay: float = 0.995,
                 epsilon_min: float = 0.01):
        
        self.state_size = state_size
        self.action_size = action_size
        self.learning_rate = learning_rate
        self.discount_factor = discount_factor
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min
        
        # Q-Tableåˆå§‹åŒ–
        self.q_table = defaultdict(lambda: np.zeros(action_size))
        
        # è®­ç»ƒå†å²
        self.training_history = {
            'rewards': [],
            'epsilon_values': [],
            'q_values_mean': [],
            'actions_taken': []
        }
    
    def get_action(self, state: int) -> int:
        """Îµ-è´ªå©ªç­–ç•¥é€‰æ‹©åŠ¨ä½œ"""
        if np.random.random() <= self.epsilon:
            # æ¢ç´¢ï¼šéšæœºé€‰æ‹©åŠ¨ä½œ
            action = np.random.choice(self.action_size)
        else:
            # åˆ©ç”¨ï¼šé€‰æ‹©Qå€¼æœ€å¤§çš„åŠ¨ä½œ
            q_values = self.q_table[state]
            action = np.argmax(q_values)
        
        self.training_history['actions_taken'].append(action)
        return action
    
    def update_q_table(self, state: int, action: int, reward: float, next_state: int):
        """æ›´æ–°Qè¡¨"""
        current_q = self.q_table[state][action]
        max_next_q = np.max(self.q_table[next_state])
        
        # Q-Learningæ›´æ–°å…¬å¼
        new_q = current_q + self.learning_rate * (
            reward + self.discount_factor * max_next_q - current_q
        )
        
        self.q_table[state][action] = new_q
        
        # è®°å½•è®­ç»ƒå†å²
        self.training_history['rewards'].append(reward)
        self.training_history['epsilon_values'].append(self.epsilon)
        
        # è®¡ç®—Qå€¼å‡å€¼
        all_q_values = []
        for state_q in self.q_table.values():
            all_q_values.extend(state_q)
        self.training_history['q_values_mean'].append(np.mean(all_q_values))
    
    def decay_epsilon(self):
        """è¡°å‡æ¢ç´¢ç‡"""
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

class AFWRLAlgorithm:
    """AFW-RLä¸»ç®—æ³•ç±»"""
    
    def __init__(self, num_nodes: int = 54, num_clusters: int = 6):
        self.num_nodes = num_nodes
        self.num_clusters = num_clusters
        
        # é¢„å®šä¹‰çš„æƒé‡åŠ¨ä½œç©ºé—´
        self.action_space = self._create_action_space()
        self.action_size = len(self.action_space)
        
        # Q-Learningæ™ºèƒ½ä½“
        self.q_agent = QLearningAgent(
            state_size=5**5,  # 5ä¸ªçŠ¶æ€ç‰¹å¾ï¼Œæ¯ä¸ª5ä¸ªç¦»æ•£å€¼
            action_size=self.action_size,
            learning_rate=0.1,
            discount_factor=0.95,
            epsilon=1.0,
            epsilon_decay=0.995,
            epsilon_min=0.01
        )
        
        # æ€§èƒ½å†å²
        self.performance_history = {
            'network_lifetime': [],
            'energy_consumption': [],
            'cluster_quality': [],
            'convergence_rounds': []
        }
        
        # å½“å‰ç½‘ç»œçŠ¶æ€
        self.current_state = None
        self.current_weights = None
        
    def _create_action_space(self) -> List[FuzzyWeightAction]:
        """åˆ›å»ºç¦»æ•£çš„æƒé‡åŠ¨ä½œç©ºé—´"""
        actions = []
        
        # é¢„å®šä¹‰çš„æƒé‡ç»„åˆï¼ˆç¡®ä¿å¤šæ ·æ€§å’Œåˆç†æ€§ï¼‰
        weight_combinations = [
            (0.7, 0.2, 0.1),  # é«˜èƒ½é‡æƒé‡
            (0.6, 0.3, 0.1),  # å¹³è¡¡èƒ½é‡-ä½ç½®
            (0.5, 0.3, 0.2),  # å‡è¡¡æƒé‡
            (0.4, 0.4, 0.2),  # ä½ç½®ä¼˜å…ˆ
            (0.3, 0.5, 0.2),  # é«˜ä½ç½®æƒé‡
            (0.4, 0.2, 0.4),  # è¿é€šæ€§ä¼˜å…ˆ
            (0.3, 0.3, 0.4),  # é«˜è¿é€šæ€§æƒé‡
            (0.5, 0.2, 0.3),  # èƒ½é‡-è¿é€šæ€§å¹³è¡¡
            (0.2, 0.4, 0.4),  # ä½ç½®-è¿é€šæ€§å¹³è¡¡
            (0.8, 0.1, 0.1),  # æé«˜èƒ½é‡æƒé‡
        ]
        
        for w_e, w_l, w_c in weight_combinations:
            actions.append(FuzzyWeightAction(w_e, w_l, w_c))
        
        return actions
    
    def calculate_network_state(self, nodes_data: np.ndarray, 
                              base_station_pos: np.ndarray) -> NetworkState:
        """è®¡ç®—å½“å‰ç½‘ç»œçŠ¶æ€"""
        # å‡è®¾nodes_dataæ ¼å¼: [x, y, energy, alive]
        alive_nodes = nodes_data[nodes_data[:, 3] > 0]
        
        if len(alive_nodes) == 0:
            return NetworkState(0, 0, 0, 0, 0)
        
        # å‰©ä½™èƒ½é‡æ¯”ä¾‹
        total_energy = np.sum(alive_nodes[:, 2])
        initial_energy = len(alive_nodes) * 1.0  # å‡è®¾åˆå§‹èƒ½é‡ä¸º1
        remaining_energy_ratio = min(total_energy / initial_energy, 1.0)
        
        # å­˜æ´»èŠ‚ç‚¹æ¯”ä¾‹
        alive_nodes_ratio = len(alive_nodes) / len(nodes_data)
        
        # ç½‘ç»œè¿é€šæ€§ï¼ˆç®€åŒ–è®¡ç®—ï¼‰
        distances = np.linalg.norm(
            alive_nodes[:, :2][:, np.newaxis] - alive_nodes[:, :2], axis=2
        )
        communication_range = 15.0  # é€šä¿¡èŒƒå›´
        connectivity_matrix = distances <= communication_range
        avg_connectivity = np.mean(np.sum(connectivity_matrix, axis=1) - 1)  # å‡å»è‡ªå·±
        network_connectivity = min(avg_connectivity / 10.0, 1.0)  # å½’ä¸€åŒ–
        
        # èƒ½é‡æ–¹å·®ï¼ˆå‡è¡¡æ€§ï¼‰
        energy_variance = np.var(alive_nodes[:, 2]) if len(alive_nodes) > 1 else 0
        energy_variance = min(energy_variance, 1.0)
        
        # å¹³å‡èŠ‚ç‚¹åº¦
        avg_node_degree = avg_connectivity
        
        return NetworkState(
            remaining_energy_ratio=remaining_energy_ratio,
            alive_nodes_ratio=alive_nodes_ratio,
            network_connectivity=network_connectivity,
            energy_variance=energy_variance,
            avg_node_degree=avg_node_degree
        )
    
    def calculate_reward(self, prev_state: NetworkState, current_state: NetworkState,
                        cluster_quality: float) -> float:
        """è®¡ç®—å¥–åŠ±å‡½æ•°"""
        # å¤šç›®æ ‡å¥–åŠ±è®¾è®¡
        
        # 1. ç½‘ç»œå¯¿å‘½å¥–åŠ±ï¼ˆå­˜æ´»èŠ‚ç‚¹æ¯”ä¾‹å˜åŒ–ï¼‰
        lifetime_reward = (current_state.alive_nodes_ratio - prev_state.alive_nodes_ratio) * 10
        
        # 2. èƒ½é‡æ•ˆç‡å¥–åŠ±ï¼ˆå‰©ä½™èƒ½é‡æ¯”ä¾‹å˜åŒ–ï¼‰
        energy_reward = (current_state.remaining_energy_ratio - prev_state.remaining_energy_ratio) * 5
        
        # 3. ç½‘ç»œè¿é€šæ€§å¥–åŠ±
        connectivity_reward = (current_state.network_connectivity - prev_state.network_connectivity) * 3
        
        # 4. èƒ½é‡å‡è¡¡å¥–åŠ±ï¼ˆæ–¹å·®è¶Šå°è¶Šå¥½ï¼‰
        balance_reward = (prev_state.energy_variance - current_state.energy_variance) * 2
        
        # 5. ç°‡è´¨é‡å¥–åŠ±
        cluster_reward = cluster_quality * 2
        
        # ç»¼åˆå¥–åŠ±
        total_reward = (lifetime_reward + energy_reward + connectivity_reward + 
                       balance_reward + cluster_reward)
        
        return total_reward
    
    def select_cluster_heads(self, nodes_data: np.ndarray, 
                           base_station_pos: np.ndarray) -> Tuple[List[int], float]:
        """ä½¿ç”¨å½“å‰æƒé‡é€‰æ‹©ç°‡å¤´"""
        if self.current_weights is None:
            # ä½¿ç”¨é»˜è®¤æƒé‡
            self.current_weights = FuzzyWeightAction(0.5, 0.3, 0.2)
        
        fuzzy_logic = FuzzyLogicClusterHead(self.current_weights)
        alive_nodes = nodes_data[nodes_data[:, 3] > 0]
        
        if len(alive_nodes) < self.num_clusters:
            # å¦‚æœå­˜æ´»èŠ‚ç‚¹å°‘äºç°‡æ•°ï¼Œå…¨éƒ¨ä½œä¸ºç°‡å¤´
            return list(range(len(alive_nodes))), 0.5
        
        # è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„æ¨¡ç³Šè¯„åˆ†
        scores = []
        for i, node in enumerate(alive_nodes):
            # èƒ½é‡æ¯”ä¾‹
            energy_ratio = node[2]  # å‡è®¾å·²å½’ä¸€åŒ–
            
            # è·ç¦»æ¯”ä¾‹
            distance = np.linalg.norm(node[:2] - base_station_pos)
            max_distance = 50.0  # å‡è®¾æœ€å¤§è·ç¦»
            distance_ratio = min(distance / max_distance, 1.0)
            
            # é‚»å±…æ•°é‡
            distances = np.linalg.norm(alive_nodes[:, :2] - node[:2], axis=1)
            neighbor_count = np.sum(distances <= 15.0) - 1  # å‡å»è‡ªå·±
            
            score = fuzzy_logic.calculate_fuzzy_score(
                energy_ratio, distance_ratio, neighbor_count
            )
            scores.append((i, score))
        
        # é€‰æ‹©è¯„åˆ†æœ€é«˜çš„èŠ‚ç‚¹ä½œä¸ºç°‡å¤´
        scores.sort(key=lambda x: x[1], reverse=True)
        cluster_heads = [scores[i][0] for i in range(min(self.num_clusters, len(scores)))]
        
        # è®¡ç®—ç°‡è´¨é‡
        cluster_quality = np.mean([scores[i][1] for i in range(len(cluster_heads))])
        
        return cluster_heads, cluster_quality
    
    def train_episode(self, nodes_data: np.ndarray, base_station_pos: np.ndarray,
                     max_rounds: int = 100) -> Dict:
        """è®­ç»ƒä¸€ä¸ªepisode"""
        episode_results = {
            'total_reward': 0,
            'rounds_survived': 0,
            'final_energy': 0,
            'cluster_quality_history': []
        }
        
        # åˆå§‹åŒ–ç½‘ç»œçŠ¶æ€
        prev_state = self.calculate_network_state(nodes_data, base_station_pos)
        prev_state_id = prev_state.discretize()
        
        for round_num in range(max_rounds):
            # é€‰æ‹©åŠ¨ä½œï¼ˆæƒé‡ç»„åˆï¼‰
            action_id = self.q_agent.get_action(prev_state_id)
            self.current_weights = self.action_space[action_id]
            
            # æ‰§è¡Œç°‡å¤´é€‰æ‹©
            cluster_heads, cluster_quality = self.select_cluster_heads(
                nodes_data, base_station_pos
            )
            
            # æ¨¡æ‹Ÿèƒ½é‡æ¶ˆè€—ï¼ˆç®€åŒ–ï¼‰
            energy_consumption = self._simulate_energy_consumption(
                nodes_data, cluster_heads, base_station_pos
            )
            
            # æ›´æ–°èŠ‚ç‚¹èƒ½é‡
            nodes_data[:, 2] -= energy_consumption
            nodes_data[nodes_data[:, 2] <= 0, 3] = 0  # æ ‡è®°æ­»äº¡èŠ‚ç‚¹
            
            # è®¡ç®—æ–°çŠ¶æ€
            current_state = self.calculate_network_state(nodes_data, base_station_pos)
            current_state_id = current_state.discretize()
            
            # è®¡ç®—å¥–åŠ±
            reward = self.calculate_reward(prev_state, current_state, cluster_quality)
            
            # æ›´æ–°Qè¡¨
            self.q_agent.update_q_table(prev_state_id, action_id, reward, current_state_id)
            
            # è®°å½•ç»“æœ
            episode_results['total_reward'] += reward
            episode_results['rounds_survived'] = round_num + 1
            episode_results['cluster_quality_history'].append(cluster_quality)
            
            # æ£€æŸ¥ç½‘ç»œæ˜¯å¦æ­»äº¡
            if current_state.alive_nodes_ratio <= 0.1:  # 90%èŠ‚ç‚¹æ­»äº¡
                break
            
            # æ›´æ–°çŠ¶æ€
            prev_state = current_state
            prev_state_id = current_state_id
        
        # è¡°å‡æ¢ç´¢ç‡
        self.q_agent.decay_epsilon()
        
        # è®°å½•æœ€ç»ˆèƒ½é‡
        episode_results['final_energy'] = np.sum(nodes_data[:, 2])
        
        return episode_results
    
    def _simulate_energy_consumption(self, nodes_data: np.ndarray, 
                                   cluster_heads: List[int], 
                                   base_station_pos: np.ndarray) -> np.ndarray:
        """æ¨¡æ‹Ÿèƒ½é‡æ¶ˆè€—"""
        energy_consumption = np.zeros(len(nodes_data))
        alive_nodes = nodes_data[nodes_data[:, 3] > 0]
        
        # åŸºç¡€æ„ŸçŸ¥èƒ½è€—
        base_sensing_energy = 0.001
        energy_consumption[nodes_data[:, 3] > 0] += base_sensing_energy
        
        # ç°‡å¤´é¢å¤–èƒ½è€—
        cluster_head_energy = 0.005
        for ch_idx in cluster_heads:
            if ch_idx < len(alive_nodes):
                original_idx = np.where(nodes_data[:, 3] > 0)[0][ch_idx]
                energy_consumption[original_idx] += cluster_head_energy
        
        # ä¼ è¾“èƒ½è€—ï¼ˆè·ç¦»ç›¸å…³ï¼‰
        transmission_energy_factor = 0.0001
        for i, node in enumerate(nodes_data):
            if node[3] > 0:  # å­˜æ´»èŠ‚ç‚¹
                distance = np.linalg.norm(node[:2] - base_station_pos)
                transmission_energy = transmission_energy_factor * (distance ** 2)
                energy_consumption[i] += transmission_energy
        
        return energy_consumption
    
    def train(self, initial_nodes_data: np.ndarray, base_station_pos: np.ndarray,
              num_episodes: int = 100) -> Dict:
        """è®­ç»ƒAFW-RLç®—æ³•"""
        print(f"ğŸš€ å¼€å§‹AFW-RLç®—æ³•è®­ç»ƒï¼Œå…±{num_episodes}ä¸ªepisodes")
        
        training_results = {
            'episode_rewards': [],
            'episode_lifetimes': [],
            'convergence_episode': None,
            'best_weights': None,
            'best_performance': 0
        }
        
        best_performance = -float('inf')
        convergence_threshold = 0.01
        recent_rewards = deque(maxlen=10)
        
        for episode in range(num_episodes):
            # é‡ç½®ç½‘ç»œçŠ¶æ€
            nodes_data = initial_nodes_data.copy()
            
            # è®­ç»ƒä¸€ä¸ªepisode
            episode_result = self.train_episode(nodes_data, base_station_pos)
            
            # è®°å½•ç»“æœ
            training_results['episode_rewards'].append(episode_result['total_reward'])
            training_results['episode_lifetimes'].append(episode_result['rounds_survived'])
            
            recent_rewards.append(episode_result['total_reward'])
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ€ä½³æ€§èƒ½
            if episode_result['total_reward'] > best_performance:
                best_performance = episode_result['total_reward']
                training_results['best_weights'] = self.current_weights
                training_results['best_performance'] = best_performance
            
            # æ£€æŸ¥æ”¶æ•›
            if len(recent_rewards) == 10:
                reward_std = np.std(recent_rewards)
                if reward_std < convergence_threshold and training_results['convergence_episode'] is None:
                    training_results['convergence_episode'] = episode
                    print(f"âœ… ç®—æ³•åœ¨ç¬¬{episode}ä¸ªepisodeæ”¶æ•›")
            
            # è¿›åº¦æŠ¥å‘Š
            if (episode + 1) % 20 == 0:
                avg_reward = np.mean(training_results['episode_rewards'][-20:])
                avg_lifetime = np.mean(training_results['episode_lifetimes'][-20:])
                print(f"Episode {episode + 1}/{num_episodes}: "
                      f"å¹³å‡å¥–åŠ±={avg_reward:.2f}, å¹³å‡å¯¿å‘½={avg_lifetime:.1f}, "
                      f"Îµ={self.q_agent.epsilon:.3f}")
        
        print(f"âœ… AFW-RLè®­ç»ƒå®Œæˆï¼æœ€ä½³æ€§èƒ½: {best_performance:.2f}")
        return training_results
    
    def evaluate(self, nodes_data: np.ndarray, base_station_pos: np.ndarray,
                max_rounds: int = 200) -> Dict:
        """è¯„ä¼°è®­ç»ƒåçš„ç®—æ³•æ€§èƒ½"""
        print("ğŸ” è¯„ä¼°AFW-RLç®—æ³•æ€§èƒ½...")
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆä¸æ¢ç´¢ï¼‰
        original_epsilon = self.q_agent.epsilon
        self.q_agent.epsilon = 0.0
        
        evaluation_results = {
            'network_lifetime': 0,
            'total_energy_consumption': 0,
            'average_cluster_quality': 0,
            'energy_efficiency': 0,
            'round_details': []
        }
        
        initial_energy = np.sum(nodes_data[:, 2])
        cluster_qualities = []
        
        for round_num in range(max_rounds):
            # è®¡ç®—ç½‘ç»œçŠ¶æ€
            current_state = self.calculate_network_state(nodes_data, base_station_pos)
            state_id = current_state.discretize()
            
            # é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
            action_id = self.q_agent.get_action(state_id)
            self.current_weights = self.action_space[action_id]
            
            # æ‰§è¡Œç°‡å¤´é€‰æ‹©
            cluster_heads, cluster_quality = self.select_cluster_heads(
                nodes_data, base_station_pos
            )
            cluster_qualities.append(cluster_quality)
            
            # æ¨¡æ‹Ÿèƒ½é‡æ¶ˆè€—
            energy_consumption = self._simulate_energy_consumption(
                nodes_data, cluster_heads, base_station_pos
            )
            
            # æ›´æ–°èŠ‚ç‚¹çŠ¶æ€
            nodes_data[:, 2] -= energy_consumption
            nodes_data[nodes_data[:, 2] <= 0, 3] = 0
            
            # è®°å½•è½®æ¬¡è¯¦æƒ…
            alive_count = np.sum(nodes_data[:, 3] > 0)
            remaining_energy = np.sum(nodes_data[:, 2])
            
            evaluation_results['round_details'].append({
                'round': round_num + 1,
                'alive_nodes': alive_count,
                'remaining_energy': remaining_energy,
                'cluster_quality': cluster_quality,
                'selected_weights': self.current_weights.to_vector().tolist()
            })
            
            # æ£€æŸ¥ç½‘ç»œæ­»äº¡
            if alive_count <= len(nodes_data) * 0.1:  # 90%èŠ‚ç‚¹æ­»äº¡
                evaluation_results['network_lifetime'] = round_num + 1
                break
        
        # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
        final_energy = np.sum(nodes_data[:, 2])
        evaluation_results['total_energy_consumption'] = initial_energy - final_energy
        evaluation_results['average_cluster_quality'] = np.mean(cluster_qualities)
        evaluation_results['energy_efficiency'] = (
            evaluation_results['network_lifetime'] / evaluation_results['total_energy_consumption']
            if evaluation_results['total_energy_consumption'] > 0 else 0
        )
        
        # æ¢å¤åŸå§‹epsilon
        self.q_agent.epsilon = original_epsilon
        
        print(f"âœ… è¯„ä¼°å®Œæˆï¼ç½‘ç»œå¯¿å‘½: {evaluation_results['network_lifetime']} è½®")
        return evaluation_results
    
    def save_model(self, filepath: str):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        model_data = {
            'q_table': dict(self.q_agent.q_table),
            'action_space': [action.to_vector().tolist() for action in self.action_space],
            'training_history': self.q_agent.training_history,
            'performance_history': self.performance_history,
            'hyperparameters': {
                'learning_rate': self.q_agent.learning_rate,
                'discount_factor': self.q_agent.discount_factor,
                'epsilon': self.q_agent.epsilon,
                'num_nodes': self.num_nodes,
                'num_clusters': self.num_clusters
            }
        }
        
        with open(filepath, 'w') as f:
            json.dump(model_data, f, indent=2)
        
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")
    
    def load_model(self, filepath: str):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        with open(filepath, 'r') as f:
            model_data = json.load(f)
        
        # æ¢å¤Qè¡¨
        self.q_agent.q_table = defaultdict(lambda: np.zeros(self.action_size))
        for state_str, q_values in model_data['q_table'].items():
            self.q_agent.q_table[int(state_str)] = np.array(q_values)
        
        # æ¢å¤å…¶ä»–å‚æ•°
        self.q_agent.training_history = model_data['training_history']
        self.performance_history = model_data['performance_history']
        
        hyperparams = model_data['hyperparameters']
        self.q_agent.learning_rate = hyperparams['learning_rate']
        self.q_agent.discount_factor = hyperparams['discount_factor']
        self.q_agent.epsilon = hyperparams['epsilon']
        
        print(f"âœ… æ¨¡å‹å·²ä» {filepath} åŠ è½½")

def demonstrate_afw_rl():
    """æ¼”ç¤ºAFW-RLç®—æ³•"""
    print("ğŸ¯ AFW-RLç®—æ³•æ¼”ç¤º")
    print("=" * 60)
    
    # ç”Ÿæˆæ¨¡æ‹Ÿç½‘ç»œæ•°æ®
    np.random.seed(42)
    num_nodes = 54
    
    # èŠ‚ç‚¹ä½ç½®ï¼ˆéšæœºåˆ†å¸ƒåœ¨50x50åŒºåŸŸï¼‰
    positions = np.random.uniform(0, 50, (num_nodes, 2))
    
    # åˆå§‹èƒ½é‡ï¼ˆå½’ä¸€åŒ–åˆ°0-1ï¼‰
    initial_energy = np.ones(num_nodes)
    
    # å­˜æ´»çŠ¶æ€ï¼ˆ1ä¸ºå­˜æ´»ï¼Œ0ä¸ºæ­»äº¡ï¼‰
    alive_status = np.ones(num_nodes)
    
    # ç»„åˆèŠ‚ç‚¹æ•°æ® [x, y, energy, alive]
    nodes_data = np.column_stack([positions, initial_energy, alive_status])
    
    # åŸºç«™ä½ç½®
    base_station_pos = np.array([25.0, 25.0])
    
    # åˆ›å»ºAFW-RLç®—æ³•å®ä¾‹
    afw_rl = AFWRLAlgorithm(num_nodes=num_nodes, num_clusters=6)
    
    # è®­ç»ƒç®—æ³•
    training_results = afw_rl.train(
        initial_nodes_data=nodes_data,
        base_station_pos=base_station_pos,
        num_episodes=50  # æ¼”ç¤ºç”¨è¾ƒå°‘episodes
    )
    
    # è¯„ä¼°ç®—æ³•
    evaluation_results = afw_rl.evaluate(
        nodes_data=nodes_data.copy(),
        base_station_pos=base_station_pos,
        max_rounds=200
    )
    
    # ä¿å­˜ç»“æœ
    results_dir = "results/afw_rl"
    import os
    os.makedirs(results_dir, exist_ok=True)
    
    # ä¿å­˜æ¨¡å‹
    afw_rl.save_model(f"{results_dir}/afw_rl_model.json")
    
    # ä¿å­˜è¯„ä¼°ç»“æœ
    with open(f"{results_dir}/evaluation_results.json", 'w') as f:
        json.dump(evaluation_results, f, indent=2)
    
    # æ‰“å°ç»“æœæ‘˜è¦
    print("\nğŸ“Š AFW-RLç®—æ³•æ€§èƒ½æ‘˜è¦")
    print("=" * 60)
    print(f"ğŸ”‹ ç½‘ç»œå¯¿å‘½: {evaluation_results['network_lifetime']} è½®")
    print(f"âš¡ æ€»èƒ½è€—: {evaluation_results['total_energy_consumption']:.3f}")
    print(f"ğŸ¯ å¹³å‡ç°‡è´¨é‡: {evaluation_results['average_cluster_quality']:.3f}")
    print(f"ğŸ“ˆ èƒ½é‡æ•ˆç‡: {evaluation_results['energy_efficiency']:.3f}")
    print(f"ğŸ† æœ€ä½³æƒé‡: {training_results['best_weights'].to_vector()}")
    print(f"ğŸ–ï¸ æ”¶æ•›è½®æ¬¡: {training_results['convergence_episode']}")
    
    return afw_rl, training_results, evaluation_results

if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    afw_rl, training_results, evaluation_results = demonstrate_afw_rl()